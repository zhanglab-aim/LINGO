{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c364732",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bf01f08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/common/zhanh/anaconda3/envs/dna_peft/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import csv\n",
    "import copy\n",
    "import json\n",
    "import logging\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional, Dict, Sequence, Tuple, List\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import transformers\n",
    "import sklearn\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, brier_score_loss\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from scipy.special import softmax\n",
    "from peftnew import (\n",
    "    LoraConfig,\n",
    "    AdaLoraConfig,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    "    AdaLoraModel\n",
    ")\n",
    "\n",
    "from transformers import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1ca610e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTrainer(Trainer):\n",
    "\n",
    "    def training_step(self, model, inputs):\n",
    "        # Call original training step\n",
    "        outputs = super().training_step(model, inputs)\n",
    "\n",
    "        if self.state.global_step % 50 == 0:\n",
    "            # Check gradients after backward pass\n",
    "            #self.check_gradients(model)\n",
    "\n",
    "            # Update and allocate\n",
    "            model.update_and_allocate(self.state.global_step)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def check_gradients(self, model):\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                if param.grad is None:\n",
    "                    print(f\"No gradient for {name}\")\n",
    "                else:\n",
    "                    gradient_norm = param.grad.norm().item()\n",
    "                    print(f\"Gradient for {name} exists with norm: {gradient_norm}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0cb03741",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainerCallback, TrainerControl, TrainerState, TrainingArguments\n",
    "\n",
    "class EvalAndSaveCallback(TrainerCallback):\n",
    "    def __init__(self, model, tokenizer, trainer):\n",
    "        self.trainer = trainer\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        if state.global_step % 500 == 0:\n",
    "            sequence = \"Domain: DNA Promoter\\nSequence: AAAAAAA\\nAnnotation:\"\n",
    "            inputs = self.tokenizer(sequence, return_tensors=\"pt\", truncation=True, padding='max_length', max_length=50)\n",
    "            inputs = {k: v.to('cuda') for k, v in inputs.items()}\n",
    "            with torch.no_grad():  # Important to use during evaluation to prevent gradient calculations\n",
    "                outputs = self.model(**inputs)\n",
    "            predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "            #predicted_label = list(label2int.keys())[list(label2int.values()).index(predictions.item())]\n",
    "            print(f\"Predicted Label at step {state.global_step}: {predictions}\")\n",
    "\n",
    "            results = self.trainer.evaluate()\n",
    "            results_file = os.path.join(args.output_dir, f\"results_step_{state.global_step}.json\")\n",
    "            output_file_path = os.path.join(args.output_dir, \"results.csv\")\n",
    "            with open(results_file, \"w\") as f:\n",
    "                json.dump(results, f)\n",
    "            with open(output_file_path, \"a\", newline='') as csv_file:\n",
    "                writer = csv.writer(csv_file)\n",
    "        \n",
    "                # Write keys (dictionary's keys) to the first row\n",
    "                if state.global_step == 500:\n",
    "                    writer.writerow(results.keys())\n",
    "\n",
    "                # Write values in the subsequent rows\n",
    "                writer.writerow(results.values())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec7ea068",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelArguments:\n",
    "    #model_name_or_path: Optional[str] = field(default=\"zhihan1996/DNABERT-2-117M\")\n",
    "    model_name_or_path: Optional[str] = field(default=\"facebook/opt-125m\")\n",
    "    #model_name_or_path: Optional[str] = field(default=\"decapoda-research/llama-7b-hf\")\n",
    "    #model_name_or_path: Optional[str] = field(default=\"microsoft/MiniLM-L12-H384-uncased\")\n",
    "    use_lora: bool = field(default=True, metadata={\"help\": \"whether to use LoRA\"})\n",
    "    lora_r: int = field(default=8, metadata={\"help\": \"hidden dimension for LoRA\"})\n",
    "    lora_alpha: int = field(default=32, metadata={\"help\": \"alpha for LoRA\"})\n",
    "    lora_dropout: float = field(default=0.05, metadata={\"help\": \"dropout rate for LoRA\"})\n",
    "    lora_target_modules: str = field(default=\"k_proj,q_proj,v_proj,fc1,fc2,output_proj\", metadata={\"help\": \"where to perform LoRA\"})\n",
    "    #lora_target_modules: str = field(default=\"Wqkv,dense,mlp.wo\", metadata={\"help\": \"where to perform LoRA\"})\n",
    "    #lora_target_modules: str = field(default=\"query,key,value\", metadata={\"help\": \"where to perform LoRA\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "253335f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataArguments:\n",
    "    data_path: str = field(default=\"/common/zhangz2lab/zhanh/GUE/prom/prom_300_all\", metadata={\"help\": \"Path to the training data.\"})\n",
    "    kmer: int = field(default=-1, metadata={\"help\": \"k-mer for input sequence. -1 means not using k-mer.\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "871af735",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "@dataclass\n",
    "class TrainingArguments(transformers.TrainingArguments):\n",
    "    cache_dir: Optional[str] = field(default=None)\n",
    "    run_name: str = field(default=\"run\")\n",
    "    optim: str = field(default=\"adamw_torch\")\n",
    "    model_max_length: int = field(default=512, metadata={\"help\": \"Maximum sequence length.\"})\n",
    "    gradient_accumulation_steps: int = field(default=1)\n",
    "    per_device_train_batch_size: int = field(default=8)\n",
    "    per_device_eval_batch_size: int = field(default=16)\n",
    "    num_train_epochs: int = field(default=5)\n",
    "    fp16: bool = field(default=False)\n",
    "    logging_steps: int = field(default=1000)\n",
    "    save_steps: int = field(default=500)\n",
    "    eval_steps: int = field(default=500)\n",
    "    evaluation_strategy: str = field(default=\"steps\")\n",
    "    load_best_model_at_end: bool = field(default=True)     # load the best model when finished training (default metric is loss)\n",
    "    metric_for_best_model: str = field(default=\"matthews_correlation\") # the metric to use to compare models\n",
    "    greater_is_better: bool = field(default=True)           # whether the `metric_for_best_model` should be maximized or not\n",
    "    logging_strategy: str = field(default=\"steps\")  # Log every \"steps\"\n",
    "    logging_steps: int = field(default=100)  # Log every 100 steps\n",
    "    warmup_ratio: int = field(default=0.1)\n",
    "    weight_decay: float = field(default=5e-3)\n",
    "    learning_rate: float = field(default=3e-5)\n",
    "    lr_scheduler_type: str = field(default='linear')\n",
    "    save_total_limit: int = field(default=10)\n",
    "    load_best_model_at_end: bool = field(default=True)\n",
    "    output_dir: str = field(default=\"/common/zhangz2lab/zhanh/output_budget_0918\")\n",
    "    find_unused_parameters: bool = field(default=False)\n",
    "    checkpointing: bool = field(default=False)\n",
    "    dataloader_pin_memory: bool = field(default=False)\n",
    "    eval_and_save_results: bool = field(default=True)\n",
    "    save_model: bool = field(default=False)\n",
    "    seed: int = field(default=42)\n",
    "    logging_first_step: bool = field(default=True)\n",
    "    early_stopping_patience: int = field(default = 5)  # number of evaluations without improvement to wait\n",
    "    early_stopping_threshold: float = field(default = 1e-3)  # threshold for an improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68d3a56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_save_model_for_hf_trainer(trainer: transformers.Trainer, output_dir: str):\n",
    "    \"\"\"Collects the state dict and dump to disk.\"\"\"\n",
    "    #state_dict = trainer.model.state_dict()\n",
    "    if trainer.args.should_save:\n",
    "        #cpu_state_dict = {key: value.cpu() for key, value in state_dict.items()}\n",
    "        #del state_dict\n",
    "        #trainer._save(output_dir, state_dict=cpu_state_dict)  # noqa\n",
    "        checkpoint_dir = os.path.join(output_dir, f\"checkpoint-{trainer.state.global_step}\")\n",
    "        trainer.model.save_pretrained(checkpoint_dir)\n",
    "        trainer.model.config.save_pretrained(checkpoint_dir) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8fd211c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_alter_of_dna_sequence(sequence: str):\n",
    "    MAP = {\"A\": \"T\", \"T\": \"A\", \"C\": \"G\", \"G\": \"C\"}\n",
    "    # return \"\".join([MAP[c] for c in reversed(sequence)])\n",
    "    return \"\".join([MAP[c] for c in sequence])\n",
    "\n",
    "\"\"\"\n",
    "Transform a dna sequence to k-mer string\n",
    "\"\"\"\n",
    "def generate_kmer_str(sequence: str, k: int) -> str:\n",
    "    \"\"\"Generate k-mer string from DNA sequence.\"\"\"\n",
    "    return \" \".join([sequence[i:i+k] for i in range(len(sequence) - k + 1)])\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Load or generate k-mer string for each DNA sequence. The generated k-mer string will be saved to the same directory as the original data with the same name but with a suffix of \"_{k}mer\".\n",
    "\"\"\"\n",
    "def load_or_generate_kmer(data_path: str, texts: List[str], k: int) -> List[str]:\n",
    "    \"\"\"Load or generate k-mer string for each DNA sequence.\"\"\"\n",
    "    kmer_path = data_path.replace(\".csv\", f\"_{k}mer.json\")\n",
    "    print(kmer_path)\n",
    "    if os.path.exists(kmer_path):\n",
    "        logging.warning(f\"Loading k-mer from {kmer_path}...\")\n",
    "        with open(kmer_path, \"r\") as f:\n",
    "            kmer = json.load(f)\n",
    "    else:        \n",
    "        logging.warning(f\"Generating k-mer...\")\n",
    "        kmer = [generate_kmer_str(text, k) for text in texts]\n",
    "        with open(kmer_path, \"w\") as f:\n",
    "            logging.warning(f\"Saving k-mer to {kmer_path}...\")\n",
    "            json.dump(kmer, f)\n",
    "        \n",
    "    return kmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7cc491b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SupervisedDataset(Dataset):\n",
    "    \"\"\"Dataset for supervised fine-tuning.\"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 data_path: str, \n",
    "                 tokenizer: transformers.PreTrainedTokenizer, \n",
    "                 kmer: int = -1):\n",
    "\n",
    "        super(SupervisedDataset, self).__init__()\n",
    "\n",
    "        # load data from the disk\n",
    "        with open(data_path, \"r\") as f:\n",
    "            data = list(csv.reader(f))[1:]\n",
    "        if len(data[0]) == 2:\n",
    "            # data is in the format of [text, label]\n",
    "            logging.warning(\"Perform single sequence classification...\")\n",
    "            #texts = [d[0] for d in data]\n",
    "            texts = [f\"Domain: DNA Promoter\\nSequence: {d[0]}\\nAnnotation:\" for d in data]\n",
    "            labels = [int(d[1]) for d in data]\n",
    "        elif len(data[0]) == 3:\n",
    "            # data is in the format of [text1, text2, label]\n",
    "            logging.warning(\"Perform sequence-pair classification...\")\n",
    "            #texts = [[d[0], d[1]] for d in data]\n",
    "            texts = [f\"Domain: DNA Promoter\\nSequence: {d[0]}\\nAnnotation:\" for d in data]\n",
    "            labels = [int(d[2]) for d in data]\n",
    "        else:\n",
    "            raise ValueError(\"Data format not supported.\")\n",
    "        \n",
    "        if kmer != -1:\n",
    "            # only write file on the first process\n",
    "            #if torch.distributed.get_rank() not in [0, -1]:\n",
    "            #    torch.distributed.barrier()\n",
    "\n",
    "            logging.warning(f\"Using {kmer}-mer as input...\")\n",
    "            texts = load_or_generate_kmer(data_path, texts, kmer)\n",
    "\n",
    "            #if torch.distributed.get_rank() == 0:\n",
    "            #    torch.distributed.barrier()\n",
    "\n",
    "        output = tokenizer(\n",
    "            texts,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"longest\",\n",
    "            max_length=tokenizer.model_max_length,\n",
    "            truncation=True,\n",
    "        )\n",
    "\n",
    "        self.input_ids = output[\"input_ids\"]\n",
    "        self.attention_mask = output[\"attention_mask\"]\n",
    "        self.labels = labels\n",
    "        self.num_labels = len(set(labels))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n",
    "        return dict(input_ids=self.input_ids[i], labels=self.labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c12ffb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollatorForSupervisedDataset(object):\n",
    "    \"\"\"Collate examples for supervised fine-tuning.\"\"\"\n",
    "\n",
    "    tokenizer: transformers.PreTrainedTokenizer\n",
    "\n",
    "    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:\n",
    "        input_ids, labels = tuple([instance[key] for instance in instances] for key in (\"input_ids\", \"labels\"))\n",
    "        input_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "            input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id\n",
    "        )\n",
    "        labels = torch.Tensor(labels).long()\n",
    "        return dict(\n",
    "            input_ids=input_ids,\n",
    "            labels=labels,\n",
    "            attention_mask=input_ids.ne(self.tokenizer.pad_token_id),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e0f1ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metric_with_sklearn(logits: np.ndarray, labels: np.ndarray):\n",
    "    if logits.ndim == 3:\n",
    "        # Reshape logits to 2D if needed\n",
    "        logits = logits.reshape(-1, logits.shape[-1])\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    valid_mask = labels != -100  # Exclude padding tokens (assuming -100 is the padding token ID)\n",
    "    valid_predictions = predictions[valid_mask]\n",
    "    valid_labels = labels[valid_mask]\n",
    "    # Compute probabilities from logits\n",
    "    probabilities = softmax(logits, axis=-1)\n",
    "\n",
    "    # Extract the probabilities corresponding to the positive class\n",
    "    valid_scores = probabilities[valid_mask, 1]  # assuming the second column is the positive class\n",
    "    return {\n",
    "        \"accuracy\": sklearn.metrics.accuracy_score(valid_labels, valid_predictions),\n",
    "        \"f1\": sklearn.metrics.f1_score(\n",
    "            valid_labels, valid_predictions, average=\"macro\", zero_division=0\n",
    "        ),\n",
    "        \"matthews_correlation\": sklearn.metrics.matthews_corrcoef(\n",
    "            valid_labels, valid_predictions\n",
    "        ),\n",
    "        \"precision\": sklearn.metrics.precision_score(\n",
    "            valid_labels, valid_predictions, average=\"macro\", zero_division=0\n",
    "        ),\n",
    "        \"recall\": sklearn.metrics.recall_score(\n",
    "            valid_labels, valid_predictions, average=\"macro\", zero_division=0\n",
    "        ),\n",
    "        \"pr_auc\": average_precision_score(valid_labels, valid_scores),\n",
    "        \"roc_auc\": roc_auc_score(valid_labels, valid_scores),\n",
    "        \"brier_score\": brier_score_loss(valid_labels, valid_scores)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b1b13d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Compute metrics used for huggingface trainer.\n",
    "\"\"\" \n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    if isinstance(logits, tuple):  # Unpack logits if it's a tuple\n",
    "        logits = logits[0]\n",
    "    return calculate_metric_with_sklearn(logits, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "55a32efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    #parser = transformers.HfArgumentParser((ModelArguments, DataArguments, TrainingArguments))\n",
    "    #print(parser)\n",
    "    #model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n",
    "    parser = transformers.HfArgumentParser((ModelArguments, DataArguments, TrainingArguments,))\n",
    "    model_args, data_args, training_args, remaining = parser.parse_args_into_dataclasses(return_remaining_strings=True)\n",
    "\n",
    "    # load tokenizer\n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "        model_args.model_name_or_path,\n",
    "        cache_dir=training_args.cache_dir,\n",
    "        model_max_length=training_args.model_max_length,\n",
    "        padding_side=\"right\",\n",
    "        use_fast=True,\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    \n",
    "\n",
    "    if \"InstaDeepAI\" in model_args.model_name_or_path:\n",
    "        tokenizer.eos_token = tokenizer.pad_token\n",
    "\n",
    "    # define datasets and data collator\n",
    "    train_dataset = SupervisedDataset(tokenizer=tokenizer, \n",
    "                                      data_path=os.path.join(data_args.data_path, \"train.csv\"), \n",
    "                                      kmer=data_args.kmer)\n",
    "    val_dataset = SupervisedDataset(tokenizer=tokenizer, \n",
    "                                     data_path=os.path.join(data_args.data_path, \"dev.csv\"), \n",
    "                                     kmer=data_args.kmer)\n",
    "    test_dataset = SupervisedDataset(tokenizer=tokenizer, \n",
    "                                     data_path=os.path.join(data_args.data_path, \"test.csv\"), \n",
    "                                     kmer=data_args.kmer)\n",
    "    data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)\n",
    "\n",
    "    #config = transformers.AutoConfig.from_pretrained(model_args.model_name_or_path, trust_remote_code=True)\n",
    "    # load model\n",
    "    model = transformers.AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_args.model_name_or_path,\n",
    "        #config = config,\n",
    "        cache_dir=training_args.cache_dir,\n",
    "        num_labels=train_dataset.num_labels,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    #module_names = [name for name, _ in model.named_modules()]\n",
    "    #print([(n, type(m)) for n, m in model.named_modules()])\n",
    "    module_names_and_types = [(n, type(m)) for n, m in model.named_modules()]\n",
    "    module = \",\".join(n for n, _ in module_names_and_types)\n",
    "    target = list(module.split(\",\"))\n",
    "    print(module)\n",
    "    \n",
    "\n",
    "    # Print the list\n",
    "    #print(module_names)\n",
    "    \n",
    "    # Get the names of the layers in the base_model\n",
    "\n",
    "    #for layer_idx, layer in enumerate(model.base_model.encoder.layer):\n",
    "        #print(f\"Layer {layer_idx}:\")\n",
    "    \n",
    "        # Get the self-attention layer\n",
    "        #self_attention_layer = layer.attention.self\n",
    "    \n",
    "        # Print the names of the sub-components\n",
    "        #for name, module in self_attention_layer.named_children():\n",
    "            #print(f\"  {name}\")\n",
    "\n",
    "    # configure LoRA\n",
    "    #model_args.lora_target_modules = r\"bert\\.encoder\\.layer\\.\\d+\\.mlp\\.wo\" \n",
    "    if model_args.use_lora:\n",
    "        lora_config = AdaLoraConfig(\n",
    "            r = model_args.lora_r,\n",
    "            init_r = 8,\n",
    "            target_r = 4,\n",
    "            tinit=200,\n",
    "            tfinal=200,\n",
    "            total_step=7485,\n",
    "            deltaT=5000,\n",
    "            #target_modules=list(r\"bert\\.encoder\\.layer\\.\\d+\\.mlp\\.wo\"),\n",
    "            lora_alpha=model_args.lora_alpha,\n",
    "            target_modules=list(model_args.lora_target_modules.split(\",\")),\n",
    "            #target_modules = target[1:],\n",
    "            lora_dropout=model_args.lora_dropout,\n",
    "            bias=\"none\",\n",
    "            task_type=\"SEQ_CLS\",\n",
    "            inference_mode=False,\n",
    "            #peft_type=\"ADALORA\",\n",
    "        )\n",
    "        print(list(model_args.lora_target_modules.split(\",\")))\n",
    "        model = get_peft_model(model, lora_config)\n",
    "        #model = AdaLoraModel(model, lora_config, \"default\")\n",
    "        model.print_trainable_parameters()\n",
    "        \n",
    "    \n",
    "\n",
    "    # define trainer\n",
    "    trainer = CustomTrainer(model=model,\n",
    "                                   tokenizer=tokenizer,\n",
    "                                   args=training_args,\n",
    "                                   compute_metrics=compute_metrics,\n",
    "                                   train_dataset=train_dataset,\n",
    "                                   eval_dataset=test_dataset,\n",
    "                                   data_collator=data_collator,\n",
    "                                  )\n",
    "    \n",
    "    callback = EvalAndSaveCallback(model, tokenizer, trainer)\n",
    "    trainer.add_callback(callback)\n",
    "\n",
    "\n",
    "    \n",
    "    #results = trainer.evaluate()\n",
    "\n",
    "    # Print or save these results if you want\n",
    "    #print(\"Initial evaluation results:\", results)\n",
    "    \n",
    "    trainer.train()\n",
    "\n",
    "    if training_args.save_model:\n",
    "        trainer.save_state()\n",
    "        safe_save_model_for_hf_trainer(trainer=trainer, output_dir=training_args.output_dir)\n",
    "\n",
    "    # get the evaluation results from trainer\n",
    "    if training_args.eval_and_save_results:\n",
    "        results_path = os.path.join(training_args.output_dir, \"results\", training_args.run_name)\n",
    "        print(results_path)\n",
    "        results = trainer.evaluate(eval_dataset=test_dataset)\n",
    "        os.makedirs(results_path, exist_ok=True)\n",
    "        with open(os.path.join(results_path, \"eval_results.json\"), \"w\") as f:\n",
    "            json.dump(results, f)\n",
    "    \n",
    "    # Get all checkpoint directories\n",
    "#     if training_args.eval_and_save_results:\n",
    "#     # Get list of all checkpoints\n",
    "#         checkpoints = [dir_name for dir_name in os.listdir(training_args.output_dir) \n",
    "#                    if 'checkpoint' in dir_name]\n",
    "    \n",
    "#         for checkpoint in checkpoints:\n",
    "#             checkpoint_path = os.path.join(training_args.output_dir, checkpoint)\n",
    "        \n",
    "#         # Load model from checkpoint\n",
    "#             model_checkpoint = transformers.AutoModelForSequenceClassification.from_pretrained(checkpoint_path,trust_remote_code=True)\n",
    "#             trainer.model = model_checkpoint  # Update trainer's model\n",
    "#             print(model.device)\n",
    "#             print(next(model.parameters()).device)\n",
    "#             for key, value in inputs.items():\n",
    "#                 if isinstance(value, torch.Tensor):\n",
    "#                     print(f\"{key} is on {value.device}\")\n",
    "#             #print(labels.device)\n",
    "#             device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#             input_ids = input_ids.to(device)\n",
    "#             token_type_ids = token_type_ids.to(device)\n",
    "#             attention_mask = attention_mask.to(device)\n",
    "\n",
    "#             results = trainer.evaluate(eval_dataset=test_dataset)\n",
    "        \n",
    "#         # Modify results_path to include checkpoint name\n",
    "#             results_path = os.path.join(training_args.output_dir, \"results\", checkpoint)\n",
    "#             os.makedirs(results_path, exist_ok=True)\n",
    "        \n",
    "#             with open(os.path.join(results_path, \"eval_results.json\"), \"w\") as f:\n",
    "#                 json.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a537eeab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "cache_dir=None,\n",
      "checkpointing=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=False,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "early_stopping_patience=5,\n",
      "early_stopping_threshold=0.001,\n",
      "eval_accumulation_steps=None,\n",
      "eval_and_save_results=True,\n",
      "eval_delay=0,\n",
      "eval_steps=500,\n",
      "evaluation_strategy=steps,\n",
      "find_unused_parameters=False,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=True,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=3e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=True,\n",
      "local_rank=-1,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=/common/zhangz2lab/zhanh/output_budget_0918/runs/Oct13_12-25-38_esplhpc-cp055,\n",
      "logging_first_step=True,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=100,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=matthews_correlation,\n",
      "model_max_length=512,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=5,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "output_dir=/common/zhangz2lab/zhanh/output_budget_0918,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=16,\n",
      "per_device_train_batch_size=8,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=run,\n",
      "save_model=False,\n",
      "save_on_each_node=False,\n",
      "save_safetensors=False,\n",
      "save_steps=500,\n",
      "save_strategy=steps,\n",
      "save_total_limit=10,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.1,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.005,\n",
      "xpu_backend=None,\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#parser = transformers.HfArgumentParser((ModelArguments, DataArguments, TrainingArguments))\n",
    "#parser.parse_args_into_dataclasses()\n",
    "\n",
    "parser = transformers.HfArgumentParser((ModelArguments, DataArguments, TrainingArguments,))\n",
    "model_args, data_args, training_args, remaining = parser.parse_args_into_dataclasses(return_remaining_strings=True)\n",
    "print(training_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe636124",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7717f87f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/opt-125m were not used when initializing OPTForSequenceClassification: ['lm_head.weight']\n",
      "- This IS expected if you are initializing OPTForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing OPTForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of OPTForSequenceClassification were not initialized from the model checkpoint at facebook/opt-125m and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ",model,model.decoder,model.decoder.embed_tokens,model.decoder.embed_positions,model.decoder.final_layer_norm,model.decoder.layers,model.decoder.layers.0,model.decoder.layers.0.self_attn,model.decoder.layers.0.self_attn.k_proj,model.decoder.layers.0.self_attn.v_proj,model.decoder.layers.0.self_attn.q_proj,model.decoder.layers.0.self_attn.out_proj,model.decoder.layers.0.activation_fn,model.decoder.layers.0.self_attn_layer_norm,model.decoder.layers.0.fc1,model.decoder.layers.0.fc2,model.decoder.layers.0.final_layer_norm,model.decoder.layers.1,model.decoder.layers.1.self_attn,model.decoder.layers.1.self_attn.k_proj,model.decoder.layers.1.self_attn.v_proj,model.decoder.layers.1.self_attn.q_proj,model.decoder.layers.1.self_attn.out_proj,model.decoder.layers.1.activation_fn,model.decoder.layers.1.self_attn_layer_norm,model.decoder.layers.1.fc1,model.decoder.layers.1.fc2,model.decoder.layers.1.final_layer_norm,model.decoder.layers.2,model.decoder.layers.2.self_attn,model.decoder.layers.2.self_attn.k_proj,model.decoder.layers.2.self_attn.v_proj,model.decoder.layers.2.self_attn.q_proj,model.decoder.layers.2.self_attn.out_proj,model.decoder.layers.2.activation_fn,model.decoder.layers.2.self_attn_layer_norm,model.decoder.layers.2.fc1,model.decoder.layers.2.fc2,model.decoder.layers.2.final_layer_norm,model.decoder.layers.3,model.decoder.layers.3.self_attn,model.decoder.layers.3.self_attn.k_proj,model.decoder.layers.3.self_attn.v_proj,model.decoder.layers.3.self_attn.q_proj,model.decoder.layers.3.self_attn.out_proj,model.decoder.layers.3.activation_fn,model.decoder.layers.3.self_attn_layer_norm,model.decoder.layers.3.fc1,model.decoder.layers.3.fc2,model.decoder.layers.3.final_layer_norm,model.decoder.layers.4,model.decoder.layers.4.self_attn,model.decoder.layers.4.self_attn.k_proj,model.decoder.layers.4.self_attn.v_proj,model.decoder.layers.4.self_attn.q_proj,model.decoder.layers.4.self_attn.out_proj,model.decoder.layers.4.activation_fn,model.decoder.layers.4.self_attn_layer_norm,model.decoder.layers.4.fc1,model.decoder.layers.4.fc2,model.decoder.layers.4.final_layer_norm,model.decoder.layers.5,model.decoder.layers.5.self_attn,model.decoder.layers.5.self_attn.k_proj,model.decoder.layers.5.self_attn.v_proj,model.decoder.layers.5.self_attn.q_proj,model.decoder.layers.5.self_attn.out_proj,model.decoder.layers.5.activation_fn,model.decoder.layers.5.self_attn_layer_norm,model.decoder.layers.5.fc1,model.decoder.layers.5.fc2,model.decoder.layers.5.final_layer_norm,model.decoder.layers.6,model.decoder.layers.6.self_attn,model.decoder.layers.6.self_attn.k_proj,model.decoder.layers.6.self_attn.v_proj,model.decoder.layers.6.self_attn.q_proj,model.decoder.layers.6.self_attn.out_proj,model.decoder.layers.6.activation_fn,model.decoder.layers.6.self_attn_layer_norm,model.decoder.layers.6.fc1,model.decoder.layers.6.fc2,model.decoder.layers.6.final_layer_norm,model.decoder.layers.7,model.decoder.layers.7.self_attn,model.decoder.layers.7.self_attn.k_proj,model.decoder.layers.7.self_attn.v_proj,model.decoder.layers.7.self_attn.q_proj,model.decoder.layers.7.self_attn.out_proj,model.decoder.layers.7.activation_fn,model.decoder.layers.7.self_attn_layer_norm,model.decoder.layers.7.fc1,model.decoder.layers.7.fc2,model.decoder.layers.7.final_layer_norm,model.decoder.layers.8,model.decoder.layers.8.self_attn,model.decoder.layers.8.self_attn.k_proj,model.decoder.layers.8.self_attn.v_proj,model.decoder.layers.8.self_attn.q_proj,model.decoder.layers.8.self_attn.out_proj,model.decoder.layers.8.activation_fn,model.decoder.layers.8.self_attn_layer_norm,model.decoder.layers.8.fc1,model.decoder.layers.8.fc2,model.decoder.layers.8.final_layer_norm,model.decoder.layers.9,model.decoder.layers.9.self_attn,model.decoder.layers.9.self_attn.k_proj,model.decoder.layers.9.self_attn.v_proj,model.decoder.layers.9.self_attn.q_proj,model.decoder.layers.9.self_attn.out_proj,model.decoder.layers.9.activation_fn,model.decoder.layers.9.self_attn_layer_norm,model.decoder.layers.9.fc1,model.decoder.layers.9.fc2,model.decoder.layers.9.final_layer_norm,model.decoder.layers.10,model.decoder.layers.10.self_attn,model.decoder.layers.10.self_attn.k_proj,model.decoder.layers.10.self_attn.v_proj,model.decoder.layers.10.self_attn.q_proj,model.decoder.layers.10.self_attn.out_proj,model.decoder.layers.10.activation_fn,model.decoder.layers.10.self_attn_layer_norm,model.decoder.layers.10.fc1,model.decoder.layers.10.fc2,model.decoder.layers.10.final_layer_norm,model.decoder.layers.11,model.decoder.layers.11.self_attn,model.decoder.layers.11.self_attn.k_proj,model.decoder.layers.11.self_attn.v_proj,model.decoder.layers.11.self_attn.q_proj,model.decoder.layers.11.self_attn.out_proj,model.decoder.layers.11.activation_fn,model.decoder.layers.11.self_attn_layer_norm,model.decoder.layers.11.fc1,model.decoder.layers.11.fc2,model.decoder.layers.11.final_layer_norm,score\n",
      "['k_proj', 'q_proj', 'v_proj', 'fc1', 'fc2', 'output_proj']\n",
      "trainable params: 1,183,200 || all params: 126,422,556 || trainable%: 0.9359089370096266\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='29600' max='29600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [29600/29600 1:04:49, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Matthews Correlation</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Pr Auc</th>\n",
       "      <th>Roc Auc</th>\n",
       "      <th>Brier Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.819200</td>\n",
       "      <td>1.621613</td>\n",
       "      <td>0.503378</td>\n",
       "      <td>0.334831</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.251689</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.540045</td>\n",
       "      <td>0.554602</td>\n",
       "      <td>0.304175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.384600</td>\n",
       "      <td>1.334353</td>\n",
       "      <td>0.568919</td>\n",
       "      <td>0.559314</td>\n",
       "      <td>0.146734</td>\n",
       "      <td>0.576960</td>\n",
       "      <td>0.569942</td>\n",
       "      <td>0.581155</td>\n",
       "      <td>0.593877</td>\n",
       "      <td>0.246394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.193400</td>\n",
       "      <td>1.097728</td>\n",
       "      <td>0.601351</td>\n",
       "      <td>0.580428</td>\n",
       "      <td>0.230861</td>\n",
       "      <td>0.629503</td>\n",
       "      <td>0.602888</td>\n",
       "      <td>0.637275</td>\n",
       "      <td>0.634630</td>\n",
       "      <td>0.243451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.871400</td>\n",
       "      <td>0.807742</td>\n",
       "      <td>0.657095</td>\n",
       "      <td>0.647229</td>\n",
       "      <td>0.336710</td>\n",
       "      <td>0.679100</td>\n",
       "      <td>0.658255</td>\n",
       "      <td>0.730259</td>\n",
       "      <td>0.695114</td>\n",
       "      <td>0.236375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.707200</td>\n",
       "      <td>0.660322</td>\n",
       "      <td>0.690372</td>\n",
       "      <td>0.690075</td>\n",
       "      <td>0.380958</td>\n",
       "      <td>0.690764</td>\n",
       "      <td>0.690194</td>\n",
       "      <td>0.813376</td>\n",
       "      <td>0.758989</td>\n",
       "      <td>0.209857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.574100</td>\n",
       "      <td>0.540286</td>\n",
       "      <td>0.771453</td>\n",
       "      <td>0.769136</td>\n",
       "      <td>0.556379</td>\n",
       "      <td>0.784347</td>\n",
       "      <td>0.772165</td>\n",
       "      <td>0.855754</td>\n",
       "      <td>0.813639</td>\n",
       "      <td>0.169997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.434700</td>\n",
       "      <td>0.519992</td>\n",
       "      <td>0.769426</td>\n",
       "      <td>0.761381</td>\n",
       "      <td>0.583751</td>\n",
       "      <td>0.814706</td>\n",
       "      <td>0.770702</td>\n",
       "      <td>0.903932</td>\n",
       "      <td>0.894803</td>\n",
       "      <td>0.163670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.408000</td>\n",
       "      <td>0.385849</td>\n",
       "      <td>0.835304</td>\n",
       "      <td>0.834762</td>\n",
       "      <td>0.676436</td>\n",
       "      <td>0.840726</td>\n",
       "      <td>0.835729</td>\n",
       "      <td>0.920867</td>\n",
       "      <td>0.921279</td>\n",
       "      <td>0.117140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.318900</td>\n",
       "      <td>0.418970</td>\n",
       "      <td>0.825676</td>\n",
       "      <td>0.823866</td>\n",
       "      <td>0.667590</td>\n",
       "      <td>0.841360</td>\n",
       "      <td>0.826398</td>\n",
       "      <td>0.930392</td>\n",
       "      <td>0.930007</td>\n",
       "      <td>0.125585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.365200</td>\n",
       "      <td>0.349505</td>\n",
       "      <td>0.859122</td>\n",
       "      <td>0.859104</td>\n",
       "      <td>0.718759</td>\n",
       "      <td>0.859523</td>\n",
       "      <td>0.859236</td>\n",
       "      <td>0.934282</td>\n",
       "      <td>0.935392</td>\n",
       "      <td>0.103227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.351700</td>\n",
       "      <td>0.345148</td>\n",
       "      <td>0.864020</td>\n",
       "      <td>0.864012</td>\n",
       "      <td>0.728026</td>\n",
       "      <td>0.864018</td>\n",
       "      <td>0.864008</td>\n",
       "      <td>0.938510</td>\n",
       "      <td>0.938785</td>\n",
       "      <td>0.100142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.316000</td>\n",
       "      <td>0.380409</td>\n",
       "      <td>0.854899</td>\n",
       "      <td>0.854465</td>\n",
       "      <td>0.715416</td>\n",
       "      <td>0.860126</td>\n",
       "      <td>0.855306</td>\n",
       "      <td>0.939404</td>\n",
       "      <td>0.940870</td>\n",
       "      <td>0.109490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.331800</td>\n",
       "      <td>0.346766</td>\n",
       "      <td>0.866723</td>\n",
       "      <td>0.866710</td>\n",
       "      <td>0.733904</td>\n",
       "      <td>0.867075</td>\n",
       "      <td>0.866830</td>\n",
       "      <td>0.942069</td>\n",
       "      <td>0.943088</td>\n",
       "      <td>0.098705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.300900</td>\n",
       "      <td>0.343070</td>\n",
       "      <td>0.872973</td>\n",
       "      <td>0.872956</td>\n",
       "      <td>0.745951</td>\n",
       "      <td>0.873016</td>\n",
       "      <td>0.872935</td>\n",
       "      <td>0.944152</td>\n",
       "      <td>0.945396</td>\n",
       "      <td>0.096387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.320100</td>\n",
       "      <td>0.362757</td>\n",
       "      <td>0.862162</td>\n",
       "      <td>0.861920</td>\n",
       "      <td>0.727942</td>\n",
       "      <td>0.865464</td>\n",
       "      <td>0.862485</td>\n",
       "      <td>0.944051</td>\n",
       "      <td>0.946108</td>\n",
       "      <td>0.103586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.317800</td>\n",
       "      <td>0.343918</td>\n",
       "      <td>0.873649</td>\n",
       "      <td>0.873637</td>\n",
       "      <td>0.747734</td>\n",
       "      <td>0.873982</td>\n",
       "      <td>0.873752</td>\n",
       "      <td>0.945441</td>\n",
       "      <td>0.947130</td>\n",
       "      <td>0.096420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.377800</td>\n",
       "      <td>0.341954</td>\n",
       "      <td>0.877196</td>\n",
       "      <td>0.877196</td>\n",
       "      <td>0.754439</td>\n",
       "      <td>0.877214</td>\n",
       "      <td>0.877226</td>\n",
       "      <td>0.946683</td>\n",
       "      <td>0.948235</td>\n",
       "      <td>0.094499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.288000</td>\n",
       "      <td>0.331447</td>\n",
       "      <td>0.880574</td>\n",
       "      <td>0.880395</td>\n",
       "      <td>0.762592</td>\n",
       "      <td>0.882241</td>\n",
       "      <td>0.880353</td>\n",
       "      <td>0.949078</td>\n",
       "      <td>0.950357</td>\n",
       "      <td>0.093280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.290800</td>\n",
       "      <td>0.407414</td>\n",
       "      <td>0.858446</td>\n",
       "      <td>0.857854</td>\n",
       "      <td>0.724539</td>\n",
       "      <td>0.865649</td>\n",
       "      <td>0.858921</td>\n",
       "      <td>0.945825</td>\n",
       "      <td>0.947078</td>\n",
       "      <td>0.109590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.328100</td>\n",
       "      <td>0.342762</td>\n",
       "      <td>0.873311</td>\n",
       "      <td>0.873216</td>\n",
       "      <td>0.748470</td>\n",
       "      <td>0.874936</td>\n",
       "      <td>0.873536</td>\n",
       "      <td>0.949054</td>\n",
       "      <td>0.950826</td>\n",
       "      <td>0.095532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.284700</td>\n",
       "      <td>0.345369</td>\n",
       "      <td>0.874155</td>\n",
       "      <td>0.874069</td>\n",
       "      <td>0.750039</td>\n",
       "      <td>0.875668</td>\n",
       "      <td>0.874372</td>\n",
       "      <td>0.949576</td>\n",
       "      <td>0.951507</td>\n",
       "      <td>0.096496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.295900</td>\n",
       "      <td>0.340122</td>\n",
       "      <td>0.877872</td>\n",
       "      <td>0.877822</td>\n",
       "      <td>0.756913</td>\n",
       "      <td>0.878866</td>\n",
       "      <td>0.878048</td>\n",
       "      <td>0.951123</td>\n",
       "      <td>0.952714</td>\n",
       "      <td>0.093692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.315700</td>\n",
       "      <td>0.328505</td>\n",
       "      <td>0.883277</td>\n",
       "      <td>0.883277</td>\n",
       "      <td>0.766631</td>\n",
       "      <td>0.883315</td>\n",
       "      <td>0.883316</td>\n",
       "      <td>0.952365</td>\n",
       "      <td>0.953935</td>\n",
       "      <td>0.090479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.323600</td>\n",
       "      <td>0.331325</td>\n",
       "      <td>0.883615</td>\n",
       "      <td>0.883522</td>\n",
       "      <td>0.767859</td>\n",
       "      <td>0.884395</td>\n",
       "      <td>0.883465</td>\n",
       "      <td>0.952390</td>\n",
       "      <td>0.953917</td>\n",
       "      <td>0.091055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.276000</td>\n",
       "      <td>0.337790</td>\n",
       "      <td>0.880236</td>\n",
       "      <td>0.880232</td>\n",
       "      <td>0.760762</td>\n",
       "      <td>0.880443</td>\n",
       "      <td>0.880319</td>\n",
       "      <td>0.951133</td>\n",
       "      <td>0.953099</td>\n",
       "      <td>0.093095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.262000</td>\n",
       "      <td>0.342561</td>\n",
       "      <td>0.881757</td>\n",
       "      <td>0.881728</td>\n",
       "      <td>0.764334</td>\n",
       "      <td>0.882432</td>\n",
       "      <td>0.881902</td>\n",
       "      <td>0.952836</td>\n",
       "      <td>0.954826</td>\n",
       "      <td>0.092252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.285700</td>\n",
       "      <td>0.361731</td>\n",
       "      <td>0.876182</td>\n",
       "      <td>0.876067</td>\n",
       "      <td>0.754583</td>\n",
       "      <td>0.878156</td>\n",
       "      <td>0.876429</td>\n",
       "      <td>0.952069</td>\n",
       "      <td>0.954053</td>\n",
       "      <td>0.097501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.246200</td>\n",
       "      <td>0.360654</td>\n",
       "      <td>0.875676</td>\n",
       "      <td>0.875531</td>\n",
       "      <td>0.753999</td>\n",
       "      <td>0.878056</td>\n",
       "      <td>0.875946</td>\n",
       "      <td>0.952794</td>\n",
       "      <td>0.954900</td>\n",
       "      <td>0.096406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>0.251600</td>\n",
       "      <td>0.326687</td>\n",
       "      <td>0.885811</td>\n",
       "      <td>0.885788</td>\n",
       "      <td>0.771674</td>\n",
       "      <td>0.885918</td>\n",
       "      <td>0.885755</td>\n",
       "      <td>0.953751</td>\n",
       "      <td>0.955362</td>\n",
       "      <td>0.089839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.300600</td>\n",
       "      <td>0.319629</td>\n",
       "      <td>0.886486</td>\n",
       "      <td>0.886486</td>\n",
       "      <td>0.773130</td>\n",
       "      <td>0.886585</td>\n",
       "      <td>0.886545</td>\n",
       "      <td>0.954713</td>\n",
       "      <td>0.956565</td>\n",
       "      <td>0.087840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>0.264500</td>\n",
       "      <td>0.322159</td>\n",
       "      <td>0.887500</td>\n",
       "      <td>0.887489</td>\n",
       "      <td>0.774995</td>\n",
       "      <td>0.887520</td>\n",
       "      <td>0.887474</td>\n",
       "      <td>0.954323</td>\n",
       "      <td>0.956105</td>\n",
       "      <td>0.088721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.274200</td>\n",
       "      <td>0.371578</td>\n",
       "      <td>0.875676</td>\n",
       "      <td>0.875507</td>\n",
       "      <td>0.754354</td>\n",
       "      <td>0.878394</td>\n",
       "      <td>0.875964</td>\n",
       "      <td>0.954093</td>\n",
       "      <td>0.955941</td>\n",
       "      <td>0.097323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>0.320200</td>\n",
       "      <td>0.323104</td>\n",
       "      <td>0.887500</td>\n",
       "      <td>0.887499</td>\n",
       "      <td>0.775028</td>\n",
       "      <td>0.887505</td>\n",
       "      <td>0.887522</td>\n",
       "      <td>0.955128</td>\n",
       "      <td>0.956794</td>\n",
       "      <td>0.089041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.262600</td>\n",
       "      <td>0.318576</td>\n",
       "      <td>0.890878</td>\n",
       "      <td>0.890841</td>\n",
       "      <td>0.781931</td>\n",
       "      <td>0.891137</td>\n",
       "      <td>0.890793</td>\n",
       "      <td>0.955928</td>\n",
       "      <td>0.957629</td>\n",
       "      <td>0.086192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>0.226300</td>\n",
       "      <td>0.324155</td>\n",
       "      <td>0.887162</td>\n",
       "      <td>0.887160</td>\n",
       "      <td>0.774541</td>\n",
       "      <td>0.887308</td>\n",
       "      <td>0.887232</td>\n",
       "      <td>0.956074</td>\n",
       "      <td>0.957730</td>\n",
       "      <td>0.087951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.264100</td>\n",
       "      <td>0.311084</td>\n",
       "      <td>0.892061</td>\n",
       "      <td>0.892019</td>\n",
       "      <td>0.784339</td>\n",
       "      <td>0.892371</td>\n",
       "      <td>0.891968</td>\n",
       "      <td>0.956045</td>\n",
       "      <td>0.957876</td>\n",
       "      <td>0.084757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18500</td>\n",
       "      <td>0.246600</td>\n",
       "      <td>0.342036</td>\n",
       "      <td>0.881926</td>\n",
       "      <td>0.881898</td>\n",
       "      <td>0.764660</td>\n",
       "      <td>0.882590</td>\n",
       "      <td>0.882070</td>\n",
       "      <td>0.956009</td>\n",
       "      <td>0.957625</td>\n",
       "      <td>0.091196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>0.290300</td>\n",
       "      <td>0.324784</td>\n",
       "      <td>0.888345</td>\n",
       "      <td>0.888344</td>\n",
       "      <td>0.776810</td>\n",
       "      <td>0.888415</td>\n",
       "      <td>0.888395</td>\n",
       "      <td>0.956939</td>\n",
       "      <td>0.958531</td>\n",
       "      <td>0.087239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19500</td>\n",
       "      <td>0.284900</td>\n",
       "      <td>0.321590</td>\n",
       "      <td>0.891385</td>\n",
       "      <td>0.891376</td>\n",
       "      <td>0.782763</td>\n",
       "      <td>0.891400</td>\n",
       "      <td>0.891363</td>\n",
       "      <td>0.956950</td>\n",
       "      <td>0.958573</td>\n",
       "      <td>0.086465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.258600</td>\n",
       "      <td>0.322048</td>\n",
       "      <td>0.891385</td>\n",
       "      <td>0.891385</td>\n",
       "      <td>0.782856</td>\n",
       "      <td>0.891429</td>\n",
       "      <td>0.891427</td>\n",
       "      <td>0.957256</td>\n",
       "      <td>0.958870</td>\n",
       "      <td>0.085779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20500</td>\n",
       "      <td>0.259600</td>\n",
       "      <td>0.315234</td>\n",
       "      <td>0.890878</td>\n",
       "      <td>0.890835</td>\n",
       "      <td>0.781980</td>\n",
       "      <td>0.891195</td>\n",
       "      <td>0.890784</td>\n",
       "      <td>0.957463</td>\n",
       "      <td>0.959010</td>\n",
       "      <td>0.084349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>0.336100</td>\n",
       "      <td>0.312015</td>\n",
       "      <td>0.890709</td>\n",
       "      <td>0.890675</td>\n",
       "      <td>0.781565</td>\n",
       "      <td>0.890935</td>\n",
       "      <td>0.890630</td>\n",
       "      <td>0.957596</td>\n",
       "      <td>0.959125</td>\n",
       "      <td>0.085786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21500</td>\n",
       "      <td>0.349500</td>\n",
       "      <td>0.306096</td>\n",
       "      <td>0.892736</td>\n",
       "      <td>0.892709</td>\n",
       "      <td>0.785572</td>\n",
       "      <td>0.892903</td>\n",
       "      <td>0.892669</td>\n",
       "      <td>0.957562</td>\n",
       "      <td>0.959270</td>\n",
       "      <td>0.083282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.302100</td>\n",
       "      <td>0.312097</td>\n",
       "      <td>0.888514</td>\n",
       "      <td>0.888513</td>\n",
       "      <td>0.777055</td>\n",
       "      <td>0.888519</td>\n",
       "      <td>0.888536</td>\n",
       "      <td>0.957395</td>\n",
       "      <td>0.959047</td>\n",
       "      <td>0.086029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22500</td>\n",
       "      <td>0.285000</td>\n",
       "      <td>0.307866</td>\n",
       "      <td>0.891385</td>\n",
       "      <td>0.891382</td>\n",
       "      <td>0.782768</td>\n",
       "      <td>0.891378</td>\n",
       "      <td>0.891390</td>\n",
       "      <td>0.957911</td>\n",
       "      <td>0.959652</td>\n",
       "      <td>0.084236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>0.257900</td>\n",
       "      <td>0.314599</td>\n",
       "      <td>0.887669</td>\n",
       "      <td>0.887667</td>\n",
       "      <td>0.775536</td>\n",
       "      <td>0.887800</td>\n",
       "      <td>0.887736</td>\n",
       "      <td>0.957722</td>\n",
       "      <td>0.959340</td>\n",
       "      <td>0.086370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23500</td>\n",
       "      <td>0.297400</td>\n",
       "      <td>0.317347</td>\n",
       "      <td>0.888851</td>\n",
       "      <td>0.888850</td>\n",
       "      <td>0.777895</td>\n",
       "      <td>0.888978</td>\n",
       "      <td>0.888917</td>\n",
       "      <td>0.957847</td>\n",
       "      <td>0.959440</td>\n",
       "      <td>0.086185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.269700</td>\n",
       "      <td>0.322098</td>\n",
       "      <td>0.886824</td>\n",
       "      <td>0.886799</td>\n",
       "      <td>0.774429</td>\n",
       "      <td>0.887463</td>\n",
       "      <td>0.886965</td>\n",
       "      <td>0.957636</td>\n",
       "      <td>0.959277</td>\n",
       "      <td>0.087973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24500</td>\n",
       "      <td>0.304600</td>\n",
       "      <td>0.296343</td>\n",
       "      <td>0.890878</td>\n",
       "      <td>0.890851</td>\n",
       "      <td>0.781850</td>\n",
       "      <td>0.891038</td>\n",
       "      <td>0.890812</td>\n",
       "      <td>0.958271</td>\n",
       "      <td>0.959598</td>\n",
       "      <td>0.084049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.235100</td>\n",
       "      <td>0.302840</td>\n",
       "      <td>0.891723</td>\n",
       "      <td>0.891716</td>\n",
       "      <td>0.783435</td>\n",
       "      <td>0.891725</td>\n",
       "      <td>0.891710</td>\n",
       "      <td>0.958266</td>\n",
       "      <td>0.959806</td>\n",
       "      <td>0.083644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25500</td>\n",
       "      <td>0.309500</td>\n",
       "      <td>0.328356</td>\n",
       "      <td>0.889020</td>\n",
       "      <td>0.888977</td>\n",
       "      <td>0.779206</td>\n",
       "      <td>0.890012</td>\n",
       "      <td>0.889194</td>\n",
       "      <td>0.957682</td>\n",
       "      <td>0.959274</td>\n",
       "      <td>0.088211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.208000</td>\n",
       "      <td>0.319656</td>\n",
       "      <td>0.889527</td>\n",
       "      <td>0.889525</td>\n",
       "      <td>0.779271</td>\n",
       "      <td>0.889674</td>\n",
       "      <td>0.889597</td>\n",
       "      <td>0.958139</td>\n",
       "      <td>0.959705</td>\n",
       "      <td>0.085918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26500</td>\n",
       "      <td>0.222500</td>\n",
       "      <td>0.318225</td>\n",
       "      <td>0.890203</td>\n",
       "      <td>0.890202</td>\n",
       "      <td>0.780542</td>\n",
       "      <td>0.890285</td>\n",
       "      <td>0.890257</td>\n",
       "      <td>0.958194</td>\n",
       "      <td>0.959731</td>\n",
       "      <td>0.086093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>0.270400</td>\n",
       "      <td>0.315483</td>\n",
       "      <td>0.889696</td>\n",
       "      <td>0.889696</td>\n",
       "      <td>0.779434</td>\n",
       "      <td>0.889710</td>\n",
       "      <td>0.889724</td>\n",
       "      <td>0.958114</td>\n",
       "      <td>0.959561</td>\n",
       "      <td>0.086244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27500</td>\n",
       "      <td>0.247500</td>\n",
       "      <td>0.306262</td>\n",
       "      <td>0.892399</td>\n",
       "      <td>0.892389</td>\n",
       "      <td>0.784790</td>\n",
       "      <td>0.892414</td>\n",
       "      <td>0.892377</td>\n",
       "      <td>0.958738</td>\n",
       "      <td>0.960169</td>\n",
       "      <td>0.084191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.259800</td>\n",
       "      <td>0.310232</td>\n",
       "      <td>0.892230</td>\n",
       "      <td>0.892228</td>\n",
       "      <td>0.784473</td>\n",
       "      <td>0.892228</td>\n",
       "      <td>0.892245</td>\n",
       "      <td>0.958650</td>\n",
       "      <td>0.960101</td>\n",
       "      <td>0.084293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28500</td>\n",
       "      <td>0.253100</td>\n",
       "      <td>0.309176</td>\n",
       "      <td>0.892568</td>\n",
       "      <td>0.892559</td>\n",
       "      <td>0.785126</td>\n",
       "      <td>0.892577</td>\n",
       "      <td>0.892549</td>\n",
       "      <td>0.958770</td>\n",
       "      <td>0.960182</td>\n",
       "      <td>0.084480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>0.281600</td>\n",
       "      <td>0.312207</td>\n",
       "      <td>0.890372</td>\n",
       "      <td>0.890371</td>\n",
       "      <td>0.780774</td>\n",
       "      <td>0.890378</td>\n",
       "      <td>0.890395</td>\n",
       "      <td>0.958710</td>\n",
       "      <td>0.960125</td>\n",
       "      <td>0.085085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29500</td>\n",
       "      <td>0.224500</td>\n",
       "      <td>0.312825</td>\n",
       "      <td>0.890372</td>\n",
       "      <td>0.890371</td>\n",
       "      <td>0.780785</td>\n",
       "      <td>0.890386</td>\n",
       "      <td>0.890400</td>\n",
       "      <td>0.958708</td>\n",
       "      <td>0.960122</td>\n",
       "      <td>0.085173</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Label at step 500: tensor([1], device='cuda:0')\n",
      "Predicted Label at step 1000: tensor([0], device='cuda:0')\n",
      "Predicted Label at step 1500: tensor([1], device='cuda:0')\n",
      "Predicted Label at step 2000: tensor([0], device='cuda:0')\n",
      "Predicted Label at step 2500: tensor([1], device='cuda:0')\n",
      "Predicted Label at step 3000: tensor([1], device='cuda:0')\n",
      "Predicted Label at step 3500: tensor([1], device='cuda:0')\n",
      "Predicted Label at step 4000: tensor([1], device='cuda:0')\n",
      "Predicted Label at step 4500: tensor([1], device='cuda:0')\n",
      "Predicted Label at step 5000: tensor([1], device='cuda:0')\n",
      "Predicted Label at step 5500: tensor([1], device='cuda:0')\n",
      "Predicted Label at step 6000: tensor([1], device='cuda:0')\n",
      "Predicted Label at step 6500: tensor([1], device='cuda:0')\n",
      "Predicted Label at step 7000: tensor([1], device='cuda:0')\n",
      "Predicted Label at step 7500: tensor([1], device='cuda:0')\n",
      "Predicted Label at step 8000: tensor([1], device='cuda:0')\n",
      "Predicted Label at step 8500: tensor([1], device='cuda:0')\n",
      "Predicted Label at step 9000: tensor([1], device='cuda:0')\n",
      "Predicted Label at step 9500: tensor([1], device='cuda:0')\n",
      "Predicted Label at step 10000: tensor([1], device='cuda:0')\n",
      "Predicted Label at step 10500: tensor([1], device='cuda:0')\n",
      "Predicted Label at step 11000: tensor([1], device='cuda:0')\n",
      "Predicted Label at step 11500: tensor([1], device='cuda:0')\n",
      "Predicted Label at step 12000: tensor([1], device='cuda:0')\n",
      "Predicted Label at step 12500: tensor([1], device='cuda:0')\n",
      "Predicted Label at step 13000: tensor([1], device='cuda:0')\n",
      "Predicted Label at step 13500: tensor([1], device='cuda:0')\n",
      "Predicted Label at step 14000: tensor([1], device='cuda:0')\n",
      "Predicted Label at step 14500: tensor([1], device='cuda:0')\n",
      "Predicted Label at step 15000: tensor([1], device='cuda:0')\n",
      "Predicted Label at step 15500: tensor([1], device='cuda:0')\n",
      "Predicted Label at step 16000: tensor([1], device='cuda:0')\n",
      "Predicted Label at step 16500: tensor([1], device='cuda:0')\n",
      "Predicted Label at step 17000: tensor([1], device='cuda:0')\n",
      "Predicted Label at step 17500: tensor([1], device='cuda:0')\n",
      "Predicted Label at step 18000: tensor([1], device='cuda:0')\n",
      "Predicted Label at step 18500: tensor([1], device='cuda:0')\n",
      "Predicted Label at step 19000: tensor([1], device='cuda:0')\n",
      "Predicted Label at step 19500: tensor([1], device='cuda:0')\n",
      "Predicted Label at step 20000: tensor([1], device='cuda:0')\n",
      "Predicted Label at step 20500: tensor([1], device='cuda:0')\n",
      "Predicted Label at step 21000: tensor([1], device='cuda:0')\n",
      "Predicted Label at step 21500: tensor([1], device='cuda:0')\n",
      "Predicted Label at step 22000: tensor([1], device='cuda:0')\n",
      "Predicted Label at step 22500: tensor([1], device='cuda:0')\n",
      "Predicted Label at step 23000: tensor([1], device='cuda:0')\n",
      "Predicted Label at step 23500: tensor([1], device='cuda:0')\n",
      "Predicted Label at step 24000: tensor([1], device='cuda:0')\n",
      "Predicted Label at step 24500: tensor([1], device='cuda:0')\n",
      "Predicted Label at step 25000: tensor([1], device='cuda:0')\n",
      "Predicted Label at step 25500: tensor([1], device='cuda:0')\n",
      "Predicted Label at step 26000: tensor([1], device='cuda:0')\n",
      "Predicted Label at step 26500: tensor([1], device='cuda:0')\n",
      "Predicted Label at step 27000: tensor([1], device='cuda:0')\n",
      "Predicted Label at step 27500: tensor([1], device='cuda:0')\n",
      "Predicted Label at step 28000: tensor([1], device='cuda:0')\n",
      "Predicted Label at step 28500: tensor([1], device='cuda:0')\n",
      "Predicted Label at step 29000: tensor([1], device='cuda:0')\n",
      "Predicted Label at step 29500: tensor([1], device='cuda:0')\n",
      "/common/zhangz2lab/zhanh/output_budget_0918/results/run\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='370' max='370' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [370/370 00:20]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2b9170",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61853275",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94662245",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279a0e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be16700",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718f3f78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DNA Peft",
   "language": "python",
   "name": "dna_peft"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
