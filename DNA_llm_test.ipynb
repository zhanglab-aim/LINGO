{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c364732",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bf01f08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/common/zhanh/anaconda3/envs/dna_peft/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import csv\n",
    "import copy\n",
    "import json\n",
    "import logging\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional, Dict, Sequence, Tuple, List\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import transformers\n",
    "import sklearn\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, brier_score_loss\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from scipy.special import softmax\n",
    "from peftnew import (\n",
    "    LoraConfig,\n",
    "    AdaLoraConfig,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    "    AdaLoraModel\n",
    ")\n",
    "\n",
    "from transformers import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1ca610e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTrainer(Trainer):\n",
    "\n",
    "    def training_step(self, model, inputs):\n",
    "        # Call original training step\n",
    "        outputs = super().training_step(model, inputs)\n",
    "\n",
    "        if self.state.global_step % 50 == 0:\n",
    "            # Check gradients after backward pass\n",
    "            #self.check_gradients(model)\n",
    "\n",
    "            # Update and allocate\n",
    "            model.update_and_allocate(self.state.global_step)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def check_gradients(self, model):\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                if param.grad is None:\n",
    "                    print(f\"No gradient for {name}\")\n",
    "                else:\n",
    "                    gradient_norm = param.grad.norm().item()\n",
    "                    print(f\"Gradient for {name} exists with norm: {gradient_norm}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0cb03741",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainerCallback, TrainerControl, TrainerState, TrainingArguments\n",
    "\n",
    "class EvalAndSaveCallback(TrainerCallback):\n",
    "    def __init__(self, model, tokenizer, trainer):\n",
    "        self.trainer = trainer\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        if state.global_step % 500 == 0:\n",
    "            results = self.trainer.evaluate()\n",
    "            results_file = os.path.join(args.output_dir, f\"results_step_{state.global_step}.json\")\n",
    "            output_file_path = os.path.join(args.output_dir, \"results.csv\")\n",
    "            with open(results_file, \"w\") as f:\n",
    "                json.dump(results, f)\n",
    "            with open(output_file_path, \"a\", newline='') as csv_file:\n",
    "                writer = csv.writer(csv_file)\n",
    "        \n",
    "                # Write keys (dictionary's keys) to the first row\n",
    "                if state.global_step == 500:\n",
    "                    writer.writerow(results.keys())\n",
    "\n",
    "                # Write values in the subsequent rows\n",
    "                writer.writerow(results.values())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec7ea068",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelArguments:\n",
    "    #model_name_or_path: Optional[str] = field(default=\"zhihan1996/DNABERT-2-117M\")\n",
    "    model_name_or_path: Optional[str] = field(default=\"facebook/opt-125m\")\n",
    "    #model_name_or_path: Optional[str] = field(default=\"decapoda-research/llama-7b-hf\")\n",
    "    #model_name_or_path: Optional[str] = field(default=\"microsoft/MiniLM-L12-H384-uncased\")\n",
    "    use_lora: bool = field(default=True, metadata={\"help\": \"whether to use LoRA\"})\n",
    "    lora_r: int = field(default=8, metadata={\"help\": \"hidden dimension for LoRA\"})\n",
    "    lora_alpha: int = field(default=32, metadata={\"help\": \"alpha for LoRA\"})\n",
    "    lora_dropout: float = field(default=0.05, metadata={\"help\": \"dropout rate for LoRA\"})\n",
    "    lora_target_modules: str = field(default=\"k_proj,q_proj,v_proj,fc1,fc2,output_proj\", metadata={\"help\": \"where to perform LoRA\"})\n",
    "    #lora_target_modules: str = field(default=\"Wqkv,dense,mlp.wo\", metadata={\"help\": \"where to perform LoRA\"})\n",
    "    #lora_target_modules: str = field(default=\"query,key,value\", metadata={\"help\": \"where to perform LoRA\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "253335f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataArguments:\n",
    "    data_path: str = field(default=\"/common/zhangz2lab/zhanh/GUE/prom/prom_300_all\", metadata={\"help\": \"Path to the training data.\"})\n",
    "    kmer: int = field(default=-1, metadata={\"help\": \"k-mer for input sequence. -1 means not using k-mer.\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "871af735",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "@dataclass\n",
    "class TrainingArguments(transformers.TrainingArguments):\n",
    "    cache_dir: Optional[str] = field(default=None)\n",
    "    run_name: str = field(default=\"run\")\n",
    "    optim: str = field(default=\"adamw_torch\")\n",
    "    model_max_length: int = field(default=512, metadata={\"help\": \"Maximum sequence length.\"})\n",
    "    gradient_accumulation_steps: int = field(default=1)\n",
    "    per_device_train_batch_size: int = field(default=8)\n",
    "    per_device_eval_batch_size: int = field(default=16)\n",
    "    num_train_epochs: int = field(default=5)\n",
    "    fp16: bool = field(default=False)\n",
    "    logging_steps: int = field(default=1000)\n",
    "    save_steps: int = field(default=500)\n",
    "    eval_steps: int = field(default=500)\n",
    "    evaluation_strategy: str = field(default=\"steps\")\n",
    "    load_best_model_at_end: bool = field(default=True)     # load the best model when finished training (default metric is loss)\n",
    "    metric_for_best_model: str = field(default=\"matthews_correlation\") # the metric to use to compare models\n",
    "    greater_is_better: bool = field(default=True)           # whether the `metric_for_best_model` should be maximized or not\n",
    "    logging_strategy: str = field(default=\"steps\")  # Log every \"steps\"\n",
    "    logging_steps: int = field(default=100)  # Log every 100 steps\n",
    "    warmup_ratio: int = field(default=0.1)\n",
    "    weight_decay: float = field(default=5e-3)\n",
    "    learning_rate: float = field(default=3e-5)\n",
    "    lr_scheduler_type: str = field(default='linear')\n",
    "    save_total_limit: int = field(default=10)\n",
    "    load_best_model_at_end: bool = field(default=True)\n",
    "    output_dir: str = field(default=\"/common/zhangz2lab/zhanh/output_budget_0918\")\n",
    "    find_unused_parameters: bool = field(default=False)\n",
    "    checkpointing: bool = field(default=False)\n",
    "    dataloader_pin_memory: bool = field(default=False)\n",
    "    eval_and_save_results: bool = field(default=True)\n",
    "    save_model: bool = field(default=False)\n",
    "    seed: int = field(default=42)\n",
    "    logging_first_step: bool = field(default=True)\n",
    "    early_stopping_patience: int = field(default = 5)  # number of evaluations without improvement to wait\n",
    "    early_stopping_threshold: float = field(default = 1e-3)  # threshold for an improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68d3a56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_save_model_for_hf_trainer(trainer: transformers.Trainer, output_dir: str):\n",
    "    \"\"\"Collects the state dict and dump to disk.\"\"\"\n",
    "    #state_dict = trainer.model.state_dict()\n",
    "    if trainer.args.should_save:\n",
    "        #cpu_state_dict = {key: value.cpu() for key, value in state_dict.items()}\n",
    "        #del state_dict\n",
    "        #trainer._save(output_dir, state_dict=cpu_state_dict)  # noqa\n",
    "        checkpoint_dir = os.path.join(output_dir, f\"checkpoint-{trainer.state.global_step}\")\n",
    "        trainer.model.save_pretrained(checkpoint_dir)\n",
    "        trainer.model.config.save_pretrained(checkpoint_dir) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8fd211c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_alter_of_dna_sequence(sequence: str):\n",
    "    MAP = {\"A\": \"T\", \"T\": \"A\", \"C\": \"G\", \"G\": \"C\"}\n",
    "    # return \"\".join([MAP[c] for c in reversed(sequence)])\n",
    "    return \"\".join([MAP[c] for c in sequence])\n",
    "\n",
    "\"\"\"\n",
    "Transform a dna sequence to k-mer string\n",
    "\"\"\"\n",
    "def generate_kmer_str(sequence: str, k: int) -> str:\n",
    "    \"\"\"Generate k-mer string from DNA sequence.\"\"\"\n",
    "    return \" \".join([sequence[i:i+k] for i in range(len(sequence) - k + 1)])\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Load or generate k-mer string for each DNA sequence. The generated k-mer string will be saved to the same directory as the original data with the same name but with a suffix of \"_{k}mer\".\n",
    "\"\"\"\n",
    "def load_or_generate_kmer(data_path: str, texts: List[str], k: int) -> List[str]:\n",
    "    \"\"\"Load or generate k-mer string for each DNA sequence.\"\"\"\n",
    "    kmer_path = data_path.replace(\".csv\", f\"_{k}mer.json\")\n",
    "    print(kmer_path)\n",
    "    if os.path.exists(kmer_path):\n",
    "        logging.warning(f\"Loading k-mer from {kmer_path}...\")\n",
    "        with open(kmer_path, \"r\") as f:\n",
    "            kmer = json.load(f)\n",
    "    else:        \n",
    "        logging.warning(f\"Generating k-mer...\")\n",
    "        kmer = [generate_kmer_str(text, k) for text in texts]\n",
    "        with open(kmer_path, \"w\") as f:\n",
    "            logging.warning(f\"Saving k-mer to {kmer_path}...\")\n",
    "            json.dump(kmer, f)\n",
    "        \n",
    "    return kmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7cc491b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SupervisedDataset(Dataset):\n",
    "    \"\"\"Dataset for supervised fine-tuning.\"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 data_path: str, \n",
    "                 tokenizer: transformers.PreTrainedTokenizer, \n",
    "                 kmer: int = -1):\n",
    "\n",
    "        super(SupervisedDataset, self).__init__()\n",
    "\n",
    "        # load data from the disk\n",
    "        with open(data_path, \"r\") as f:\n",
    "            data = list(csv.reader(f))[1:]\n",
    "        if len(data[0]) == 2:\n",
    "            # data is in the format of [text, label]\n",
    "            logging.warning(\"Perform single sequence classification...\")\n",
    "            texts = [d[0] for d in data]\n",
    "            labels = [int(d[1]) for d in data]\n",
    "        elif len(data[0]) == 3:\n",
    "            # data is in the format of [text1, text2, label]\n",
    "            logging.warning(\"Perform sequence-pair classification...\")\n",
    "            texts = [[d[0], d[1]] for d in data]\n",
    "            labels = [int(d[2]) for d in data]\n",
    "        else:\n",
    "            raise ValueError(\"Data format not supported.\")\n",
    "        \n",
    "        if kmer != -1:\n",
    "            # only write file on the first process\n",
    "            #if torch.distributed.get_rank() not in [0, -1]:\n",
    "            #    torch.distributed.barrier()\n",
    "\n",
    "            logging.warning(f\"Using {kmer}-mer as input...\")\n",
    "            texts = load_or_generate_kmer(data_path, texts, kmer)\n",
    "\n",
    "            #if torch.distributed.get_rank() == 0:\n",
    "            #    torch.distributed.barrier()\n",
    "\n",
    "        output = tokenizer(\n",
    "            texts,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"longest\",\n",
    "            max_length=tokenizer.model_max_length,\n",
    "            truncation=True,\n",
    "        )\n",
    "\n",
    "        self.input_ids = output[\"input_ids\"]\n",
    "        self.attention_mask = output[\"attention_mask\"]\n",
    "        self.labels = labels\n",
    "        self.num_labels = len(set(labels))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n",
    "        return dict(input_ids=self.input_ids[i], labels=self.labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c12ffb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollatorForSupervisedDataset(object):\n",
    "    \"\"\"Collate examples for supervised fine-tuning.\"\"\"\n",
    "\n",
    "    tokenizer: transformers.PreTrainedTokenizer\n",
    "\n",
    "    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:\n",
    "        input_ids, labels = tuple([instance[key] for instance in instances] for key in (\"input_ids\", \"labels\"))\n",
    "        input_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "            input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id\n",
    "        )\n",
    "        labels = torch.Tensor(labels).long()\n",
    "        return dict(\n",
    "            input_ids=input_ids,\n",
    "            labels=labels,\n",
    "            attention_mask=input_ids.ne(self.tokenizer.pad_token_id),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e0f1ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metric_with_sklearn(logits: np.ndarray, labels: np.ndarray):\n",
    "    if logits.ndim == 3:\n",
    "        # Reshape logits to 2D if needed\n",
    "        logits = logits.reshape(-1, logits.shape[-1])\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    valid_mask = labels != -100  # Exclude padding tokens (assuming -100 is the padding token ID)\n",
    "    valid_predictions = predictions[valid_mask]\n",
    "    valid_labels = labels[valid_mask]\n",
    "    # Compute probabilities from logits\n",
    "    probabilities = softmax(logits, axis=-1)\n",
    "\n",
    "    # Extract the probabilities corresponding to the positive class\n",
    "    valid_scores = probabilities[valid_mask, 1]  # assuming the second column is the positive class\n",
    "    return {\n",
    "        \"accuracy\": sklearn.metrics.accuracy_score(valid_labels, valid_predictions),\n",
    "        \"f1\": sklearn.metrics.f1_score(\n",
    "            valid_labels, valid_predictions, average=\"macro\", zero_division=0\n",
    "        ),\n",
    "        \"matthews_correlation\": sklearn.metrics.matthews_corrcoef(\n",
    "            valid_labels, valid_predictions\n",
    "        ),\n",
    "        \"precision\": sklearn.metrics.precision_score(\n",
    "            valid_labels, valid_predictions, average=\"macro\", zero_division=0\n",
    "        ),\n",
    "        \"recall\": sklearn.metrics.recall_score(\n",
    "            valid_labels, valid_predictions, average=\"macro\", zero_division=0\n",
    "        ),\n",
    "        \"pr_auc\": average_precision_score(valid_labels, valid_scores),\n",
    "        \"roc_auc\": roc_auc_score(valid_labels, valid_scores),\n",
    "        \"brier_score\": brier_score_loss(valid_labels, valid_scores)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b1b13d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Compute metrics used for huggingface trainer.\n",
    "\"\"\" \n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    if isinstance(logits, tuple):  # Unpack logits if it's a tuple\n",
    "        logits = logits[0]\n",
    "    return calculate_metric_with_sklearn(logits, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "55a32efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    #parser = transformers.HfArgumentParser((ModelArguments, DataArguments, TrainingArguments))\n",
    "    #print(parser)\n",
    "    #model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n",
    "    parser = transformers.HfArgumentParser((ModelArguments, DataArguments, TrainingArguments,))\n",
    "    model_args, data_args, training_args, remaining = parser.parse_args_into_dataclasses(return_remaining_strings=True)\n",
    "\n",
    "    # load tokenizer\n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "        model_args.model_name_or_path,\n",
    "        cache_dir=training_args.cache_dir,\n",
    "        model_max_length=training_args.model_max_length,\n",
    "        padding_side=\"right\",\n",
    "        use_fast=True,\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    \n",
    "\n",
    "    if \"InstaDeepAI\" in model_args.model_name_or_path:\n",
    "        tokenizer.eos_token = tokenizer.pad_token\n",
    "\n",
    "    # define datasets and data collator\n",
    "    train_dataset = SupervisedDataset(tokenizer=tokenizer, \n",
    "                                      data_path=os.path.join(data_args.data_path, \"train.csv\"), \n",
    "                                      kmer=data_args.kmer)\n",
    "    val_dataset = SupervisedDataset(tokenizer=tokenizer, \n",
    "                                     data_path=os.path.join(data_args.data_path, \"dev.csv\"), \n",
    "                                     kmer=data_args.kmer)\n",
    "    test_dataset = SupervisedDataset(tokenizer=tokenizer, \n",
    "                                     data_path=os.path.join(data_args.data_path, \"test.csv\"), \n",
    "                                     kmer=data_args.kmer)\n",
    "    data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)\n",
    "\n",
    "    #config = transformers.AutoConfig.from_pretrained(model_args.model_name_or_path, trust_remote_code=True)\n",
    "    # load model\n",
    "    model = transformers.AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_args.model_name_or_path,\n",
    "        #config = config,\n",
    "        cache_dir=training_args.cache_dir,\n",
    "        num_labels=train_dataset.num_labels,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    #module_names = [name for name, _ in model.named_modules()]\n",
    "    #print([(n, type(m)) for n, m in model.named_modules()])\n",
    "    module_names_and_types = [(n, type(m)) for n, m in model.named_modules()]\n",
    "    module = \",\".join(n for n, _ in module_names_and_types)\n",
    "    target = list(module.split(\",\"))\n",
    "    print(module)\n",
    "    \n",
    "\n",
    "    # Print the list\n",
    "    #print(module_names)\n",
    "    \n",
    "    # Get the names of the layers in the base_model\n",
    "\n",
    "    #for layer_idx, layer in enumerate(model.base_model.encoder.layer):\n",
    "        #print(f\"Layer {layer_idx}:\")\n",
    "    \n",
    "        # Get the self-attention layer\n",
    "        #self_attention_layer = layer.attention.self\n",
    "    \n",
    "        # Print the names of the sub-components\n",
    "        #for name, module in self_attention_layer.named_children():\n",
    "            #print(f\"  {name}\")\n",
    "\n",
    "    # configure LoRA\n",
    "    #model_args.lora_target_modules = r\"bert\\.encoder\\.layer\\.\\d+\\.mlp\\.wo\" \n",
    "    if model_args.use_lora:\n",
    "        lora_config = AdaLoraConfig(\n",
    "            r = model_args.lora_r,\n",
    "            init_r = 8,\n",
    "            target_r = 4,\n",
    "            tinit=200,\n",
    "            tfinal=200,\n",
    "            total_step=7485,\n",
    "            deltaT=5000,\n",
    "            #target_modules=list(r\"bert\\.encoder\\.layer\\.\\d+\\.mlp\\.wo\"),\n",
    "            lora_alpha=model_args.lora_alpha,\n",
    "            target_modules=list(model_args.lora_target_modules.split(\",\")),\n",
    "            #target_modules = target[1:],\n",
    "            lora_dropout=model_args.lora_dropout,\n",
    "            bias=\"none\",\n",
    "            task_type=\"SEQ_CLS\",\n",
    "            inference_mode=False,\n",
    "            #peft_type=\"ADALORA\",\n",
    "        )\n",
    "        print(list(model_args.lora_target_modules.split(\",\")))\n",
    "        model = get_peft_model(model, lora_config)\n",
    "        #model = AdaLoraModel(model, lora_config, \"default\")\n",
    "        model.print_trainable_parameters()\n",
    "        \n",
    "    \n",
    "\n",
    "    # define trainer\n",
    "    trainer = CustomTrainer(model=model,\n",
    "                                   tokenizer=tokenizer,\n",
    "                                   args=training_args,\n",
    "                                   compute_metrics=compute_metrics,\n",
    "                                   train_dataset=train_dataset,\n",
    "                                   eval_dataset=test_dataset,\n",
    "                                   data_collator=data_collator,\n",
    "                                  )\n",
    "    \n",
    "    callback = EvalAndSaveCallback(model, tokenizer, trainer)\n",
    "    trainer.add_callback(callback)\n",
    "\n",
    "\n",
    "    \n",
    "    #results = trainer.evaluate()\n",
    "\n",
    "    # Print or save these results if you want\n",
    "    #print(\"Initial evaluation results:\", results)\n",
    "    \n",
    "    trainer.train()\n",
    "\n",
    "    if training_args.save_model:\n",
    "        trainer.save_state()\n",
    "        safe_save_model_for_hf_trainer(trainer=trainer, output_dir=training_args.output_dir)\n",
    "\n",
    "    # get the evaluation results from trainer\n",
    "    if training_args.eval_and_save_results:\n",
    "        results_path = os.path.join(training_args.output_dir, \"results\", training_args.run_name)\n",
    "        print(results_path)\n",
    "        results = trainer.evaluate(eval_dataset=test_dataset)\n",
    "        os.makedirs(results_path, exist_ok=True)\n",
    "        with open(os.path.join(results_path, \"eval_results.json\"), \"w\") as f:\n",
    "            json.dump(results, f)\n",
    "    \n",
    "    # Get all checkpoint directories\n",
    "#     if training_args.eval_and_save_results:\n",
    "#     # Get list of all checkpoints\n",
    "#         checkpoints = [dir_name for dir_name in os.listdir(training_args.output_dir) \n",
    "#                    if 'checkpoint' in dir_name]\n",
    "    \n",
    "#         for checkpoint in checkpoints:\n",
    "#             checkpoint_path = os.path.join(training_args.output_dir, checkpoint)\n",
    "        \n",
    "#         # Load model from checkpoint\n",
    "#             model_checkpoint = transformers.AutoModelForSequenceClassification.from_pretrained(checkpoint_path,trust_remote_code=True)\n",
    "#             trainer.model = model_checkpoint  # Update trainer's model\n",
    "#             print(model.device)\n",
    "#             print(next(model.parameters()).device)\n",
    "#             for key, value in inputs.items():\n",
    "#                 if isinstance(value, torch.Tensor):\n",
    "#                     print(f\"{key} is on {value.device}\")\n",
    "#             #print(labels.device)\n",
    "#             device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#             input_ids = input_ids.to(device)\n",
    "#             token_type_ids = token_type_ids.to(device)\n",
    "#             attention_mask = attention_mask.to(device)\n",
    "\n",
    "#             results = trainer.evaluate(eval_dataset=test_dataset)\n",
    "        \n",
    "#         # Modify results_path to include checkpoint name\n",
    "#             results_path = os.path.join(training_args.output_dir, \"results\", checkpoint)\n",
    "#             os.makedirs(results_path, exist_ok=True)\n",
    "        \n",
    "#             with open(os.path.join(results_path, \"eval_results.json\"), \"w\") as f:\n",
    "#                 json.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a537eeab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "cache_dir=None,\n",
      "checkpointing=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=False,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "early_stopping_patience=5,\n",
      "early_stopping_threshold=0.001,\n",
      "eval_accumulation_steps=None,\n",
      "eval_and_save_results=True,\n",
      "eval_delay=0,\n",
      "eval_steps=500,\n",
      "evaluation_strategy=steps,\n",
      "find_unused_parameters=False,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=True,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=3e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=True,\n",
      "local_rank=-1,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=/common/zhangz2lab/zhanh/output_budget_0918/runs/Sep18_16-57-46_esplhpc-cp055,\n",
      "logging_first_step=True,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=100,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=matthews_correlation,\n",
      "model_max_length=512,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=5,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "output_dir=/common/zhangz2lab/zhanh/output_budget_0918,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=16,\n",
      "per_device_train_batch_size=8,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=run,\n",
      "save_model=False,\n",
      "save_on_each_node=False,\n",
      "save_safetensors=False,\n",
      "save_steps=500,\n",
      "save_strategy=steps,\n",
      "save_total_limit=10,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.1,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.005,\n",
      "xpu_backend=None,\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#parser = transformers.HfArgumentParser((ModelArguments, DataArguments, TrainingArguments))\n",
    "#parser.parse_args_into_dataclasses()\n",
    "\n",
    "parser = transformers.HfArgumentParser((ModelArguments, DataArguments, TrainingArguments,))\n",
    "model_args, data_args, training_args, remaining = parser.parse_args_into_dataclasses(return_remaining_strings=True)\n",
    "print(training_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe636124",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7717f87f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/opt-125m were not used when initializing OPTForSequenceClassification: ['lm_head.weight']\n",
      "- This IS expected if you are initializing OPTForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing OPTForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of OPTForSequenceClassification were not initialized from the model checkpoint at facebook/opt-125m and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ",model,model.decoder,model.decoder.embed_tokens,model.decoder.embed_positions,model.decoder.final_layer_norm,model.decoder.layers,model.decoder.layers.0,model.decoder.layers.0.self_attn,model.decoder.layers.0.self_attn.k_proj,model.decoder.layers.0.self_attn.v_proj,model.decoder.layers.0.self_attn.q_proj,model.decoder.layers.0.self_attn.out_proj,model.decoder.layers.0.activation_fn,model.decoder.layers.0.self_attn_layer_norm,model.decoder.layers.0.fc1,model.decoder.layers.0.fc2,model.decoder.layers.0.final_layer_norm,model.decoder.layers.1,model.decoder.layers.1.self_attn,model.decoder.layers.1.self_attn.k_proj,model.decoder.layers.1.self_attn.v_proj,model.decoder.layers.1.self_attn.q_proj,model.decoder.layers.1.self_attn.out_proj,model.decoder.layers.1.activation_fn,model.decoder.layers.1.self_attn_layer_norm,model.decoder.layers.1.fc1,model.decoder.layers.1.fc2,model.decoder.layers.1.final_layer_norm,model.decoder.layers.2,model.decoder.layers.2.self_attn,model.decoder.layers.2.self_attn.k_proj,model.decoder.layers.2.self_attn.v_proj,model.decoder.layers.2.self_attn.q_proj,model.decoder.layers.2.self_attn.out_proj,model.decoder.layers.2.activation_fn,model.decoder.layers.2.self_attn_layer_norm,model.decoder.layers.2.fc1,model.decoder.layers.2.fc2,model.decoder.layers.2.final_layer_norm,model.decoder.layers.3,model.decoder.layers.3.self_attn,model.decoder.layers.3.self_attn.k_proj,model.decoder.layers.3.self_attn.v_proj,model.decoder.layers.3.self_attn.q_proj,model.decoder.layers.3.self_attn.out_proj,model.decoder.layers.3.activation_fn,model.decoder.layers.3.self_attn_layer_norm,model.decoder.layers.3.fc1,model.decoder.layers.3.fc2,model.decoder.layers.3.final_layer_norm,model.decoder.layers.4,model.decoder.layers.4.self_attn,model.decoder.layers.4.self_attn.k_proj,model.decoder.layers.4.self_attn.v_proj,model.decoder.layers.4.self_attn.q_proj,model.decoder.layers.4.self_attn.out_proj,model.decoder.layers.4.activation_fn,model.decoder.layers.4.self_attn_layer_norm,model.decoder.layers.4.fc1,model.decoder.layers.4.fc2,model.decoder.layers.4.final_layer_norm,model.decoder.layers.5,model.decoder.layers.5.self_attn,model.decoder.layers.5.self_attn.k_proj,model.decoder.layers.5.self_attn.v_proj,model.decoder.layers.5.self_attn.q_proj,model.decoder.layers.5.self_attn.out_proj,model.decoder.layers.5.activation_fn,model.decoder.layers.5.self_attn_layer_norm,model.decoder.layers.5.fc1,model.decoder.layers.5.fc2,model.decoder.layers.5.final_layer_norm,model.decoder.layers.6,model.decoder.layers.6.self_attn,model.decoder.layers.6.self_attn.k_proj,model.decoder.layers.6.self_attn.v_proj,model.decoder.layers.6.self_attn.q_proj,model.decoder.layers.6.self_attn.out_proj,model.decoder.layers.6.activation_fn,model.decoder.layers.6.self_attn_layer_norm,model.decoder.layers.6.fc1,model.decoder.layers.6.fc2,model.decoder.layers.6.final_layer_norm,model.decoder.layers.7,model.decoder.layers.7.self_attn,model.decoder.layers.7.self_attn.k_proj,model.decoder.layers.7.self_attn.v_proj,model.decoder.layers.7.self_attn.q_proj,model.decoder.layers.7.self_attn.out_proj,model.decoder.layers.7.activation_fn,model.decoder.layers.7.self_attn_layer_norm,model.decoder.layers.7.fc1,model.decoder.layers.7.fc2,model.decoder.layers.7.final_layer_norm,model.decoder.layers.8,model.decoder.layers.8.self_attn,model.decoder.layers.8.self_attn.k_proj,model.decoder.layers.8.self_attn.v_proj,model.decoder.layers.8.self_attn.q_proj,model.decoder.layers.8.self_attn.out_proj,model.decoder.layers.8.activation_fn,model.decoder.layers.8.self_attn_layer_norm,model.decoder.layers.8.fc1,model.decoder.layers.8.fc2,model.decoder.layers.8.final_layer_norm,model.decoder.layers.9,model.decoder.layers.9.self_attn,model.decoder.layers.9.self_attn.k_proj,model.decoder.layers.9.self_attn.v_proj,model.decoder.layers.9.self_attn.q_proj,model.decoder.layers.9.self_attn.out_proj,model.decoder.layers.9.activation_fn,model.decoder.layers.9.self_attn_layer_norm,model.decoder.layers.9.fc1,model.decoder.layers.9.fc2,model.decoder.layers.9.final_layer_norm,model.decoder.layers.10,model.decoder.layers.10.self_attn,model.decoder.layers.10.self_attn.k_proj,model.decoder.layers.10.self_attn.v_proj,model.decoder.layers.10.self_attn.q_proj,model.decoder.layers.10.self_attn.out_proj,model.decoder.layers.10.activation_fn,model.decoder.layers.10.self_attn_layer_norm,model.decoder.layers.10.fc1,model.decoder.layers.10.fc2,model.decoder.layers.10.final_layer_norm,model.decoder.layers.11,model.decoder.layers.11.self_attn,model.decoder.layers.11.self_attn.k_proj,model.decoder.layers.11.self_attn.v_proj,model.decoder.layers.11.self_attn.q_proj,model.decoder.layers.11.self_attn.out_proj,model.decoder.layers.11.activation_fn,model.decoder.layers.11.self_attn_layer_norm,model.decoder.layers.11.fc1,model.decoder.layers.11.fc2,model.decoder.layers.11.final_layer_norm,score\n",
      "['k_proj', 'q_proj', 'v_proj', 'fc1', 'fc2', 'output_proj']\n",
      "trainable params: 1,183,200 || all params: 126,422,556 || trainable%: 0.9359089370096266\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='29600' max='29600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [29600/29600 59:42, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Matthews Correlation</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Pr Auc</th>\n",
       "      <th>Roc Auc</th>\n",
       "      <th>Brier Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.601900</td>\n",
       "      <td>1.496253</td>\n",
       "      <td>0.547297</td>\n",
       "      <td>0.472126</td>\n",
       "      <td>0.153880</td>\n",
       "      <td>0.618699</td>\n",
       "      <td>0.549872</td>\n",
       "      <td>0.609154</td>\n",
       "      <td>0.592983</td>\n",
       "      <td>0.253363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.354700</td>\n",
       "      <td>1.313025</td>\n",
       "      <td>0.579730</td>\n",
       "      <td>0.578046</td>\n",
       "      <td>0.159802</td>\n",
       "      <td>0.580477</td>\n",
       "      <td>0.579329</td>\n",
       "      <td>0.646631</td>\n",
       "      <td>0.625139</td>\n",
       "      <td>0.237999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.163700</td>\n",
       "      <td>1.065053</td>\n",
       "      <td>0.613514</td>\n",
       "      <td>0.612979</td>\n",
       "      <td>0.227095</td>\n",
       "      <td>0.613805</td>\n",
       "      <td>0.613290</td>\n",
       "      <td>0.704194</td>\n",
       "      <td>0.672865</td>\n",
       "      <td>0.229756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.802200</td>\n",
       "      <td>0.753646</td>\n",
       "      <td>0.663176</td>\n",
       "      <td>0.662936</td>\n",
       "      <td>0.326399</td>\n",
       "      <td>0.663373</td>\n",
       "      <td>0.663026</td>\n",
       "      <td>0.764065</td>\n",
       "      <td>0.719570</td>\n",
       "      <td>0.217542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.637900</td>\n",
       "      <td>0.639208</td>\n",
       "      <td>0.643919</td>\n",
       "      <td>0.637363</td>\n",
       "      <td>0.296441</td>\n",
       "      <td>0.653589</td>\n",
       "      <td>0.643040</td>\n",
       "      <td>0.807324</td>\n",
       "      <td>0.754764</td>\n",
       "      <td>0.207966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.556800</td>\n",
       "      <td>0.528165</td>\n",
       "      <td>0.758277</td>\n",
       "      <td>0.757876</td>\n",
       "      <td>0.519184</td>\n",
       "      <td>0.760601</td>\n",
       "      <td>0.758587</td>\n",
       "      <td>0.853541</td>\n",
       "      <td>0.814722</td>\n",
       "      <td>0.169138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.434200</td>\n",
       "      <td>0.476471</td>\n",
       "      <td>0.786486</td>\n",
       "      <td>0.781862</td>\n",
       "      <td>0.602362</td>\n",
       "      <td>0.815506</td>\n",
       "      <td>0.787506</td>\n",
       "      <td>0.902066</td>\n",
       "      <td>0.889765</td>\n",
       "      <td>0.150568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.420800</td>\n",
       "      <td>0.386038</td>\n",
       "      <td>0.837669</td>\n",
       "      <td>0.837582</td>\n",
       "      <td>0.675614</td>\n",
       "      <td>0.838064</td>\n",
       "      <td>0.837551</td>\n",
       "      <td>0.921105</td>\n",
       "      <td>0.916697</td>\n",
       "      <td>0.116961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.323700</td>\n",
       "      <td>0.390472</td>\n",
       "      <td>0.841723</td>\n",
       "      <td>0.841279</td>\n",
       "      <td>0.688593</td>\n",
       "      <td>0.846488</td>\n",
       "      <td>0.842119</td>\n",
       "      <td>0.928758</td>\n",
       "      <td>0.926934</td>\n",
       "      <td>0.114629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.333000</td>\n",
       "      <td>0.397082</td>\n",
       "      <td>0.848986</td>\n",
       "      <td>0.848457</td>\n",
       "      <td>0.701609</td>\n",
       "      <td>0.852997</td>\n",
       "      <td>0.848626</td>\n",
       "      <td>0.932629</td>\n",
       "      <td>0.931783</td>\n",
       "      <td>0.112760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.348900</td>\n",
       "      <td>0.405056</td>\n",
       "      <td>0.846115</td>\n",
       "      <td>0.845184</td>\n",
       "      <td>0.698973</td>\n",
       "      <td>0.853387</td>\n",
       "      <td>0.845630</td>\n",
       "      <td>0.935321</td>\n",
       "      <td>0.934616</td>\n",
       "      <td>0.114431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.317200</td>\n",
       "      <td>0.365990</td>\n",
       "      <td>0.861149</td>\n",
       "      <td>0.861066</td>\n",
       "      <td>0.723800</td>\n",
       "      <td>0.862448</td>\n",
       "      <td>0.861352</td>\n",
       "      <td>0.937858</td>\n",
       "      <td>0.938129</td>\n",
       "      <td>0.105171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.322200</td>\n",
       "      <td>0.364521</td>\n",
       "      <td>0.865372</td>\n",
       "      <td>0.865133</td>\n",
       "      <td>0.732432</td>\n",
       "      <td>0.867309</td>\n",
       "      <td>0.865127</td>\n",
       "      <td>0.940424</td>\n",
       "      <td>0.940653</td>\n",
       "      <td>0.102356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.293300</td>\n",
       "      <td>0.365163</td>\n",
       "      <td>0.866554</td>\n",
       "      <td>0.866444</td>\n",
       "      <td>0.733725</td>\n",
       "      <td>0.867326</td>\n",
       "      <td>0.866400</td>\n",
       "      <td>0.941825</td>\n",
       "      <td>0.942057</td>\n",
       "      <td>0.100669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.297100</td>\n",
       "      <td>0.345386</td>\n",
       "      <td>0.869426</td>\n",
       "      <td>0.869419</td>\n",
       "      <td>0.739179</td>\n",
       "      <td>0.869665</td>\n",
       "      <td>0.869514</td>\n",
       "      <td>0.942709</td>\n",
       "      <td>0.943532</td>\n",
       "      <td>0.098201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.329000</td>\n",
       "      <td>0.349670</td>\n",
       "      <td>0.872804</td>\n",
       "      <td>0.872680</td>\n",
       "      <td>0.746409</td>\n",
       "      <td>0.873777</td>\n",
       "      <td>0.872633</td>\n",
       "      <td>0.944151</td>\n",
       "      <td>0.944895</td>\n",
       "      <td>0.097627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.379700</td>\n",
       "      <td>0.373863</td>\n",
       "      <td>0.869088</td>\n",
       "      <td>0.868543</td>\n",
       "      <td>0.742914</td>\n",
       "      <td>0.874242</td>\n",
       "      <td>0.868693</td>\n",
       "      <td>0.945047</td>\n",
       "      <td>0.945787</td>\n",
       "      <td>0.102583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.303500</td>\n",
       "      <td>0.425510</td>\n",
       "      <td>0.851520</td>\n",
       "      <td>0.849943</td>\n",
       "      <td>0.715936</td>\n",
       "      <td>0.865213</td>\n",
       "      <td>0.850866</td>\n",
       "      <td>0.946046</td>\n",
       "      <td>0.946408</td>\n",
       "      <td>0.115552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.285400</td>\n",
       "      <td>0.348468</td>\n",
       "      <td>0.873818</td>\n",
       "      <td>0.873807</td>\n",
       "      <td>0.748064</td>\n",
       "      <td>0.874143</td>\n",
       "      <td>0.873920</td>\n",
       "      <td>0.946085</td>\n",
       "      <td>0.946737</td>\n",
       "      <td>0.096915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.346200</td>\n",
       "      <td>0.330255</td>\n",
       "      <td>0.878547</td>\n",
       "      <td>0.878478</td>\n",
       "      <td>0.757469</td>\n",
       "      <td>0.879043</td>\n",
       "      <td>0.878426</td>\n",
       "      <td>0.948132</td>\n",
       "      <td>0.948984</td>\n",
       "      <td>0.092791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.294100</td>\n",
       "      <td>0.330405</td>\n",
       "      <td>0.878885</td>\n",
       "      <td>0.878879</td>\n",
       "      <td>0.757758</td>\n",
       "      <td>0.878882</td>\n",
       "      <td>0.878876</td>\n",
       "      <td>0.948119</td>\n",
       "      <td>0.949299</td>\n",
       "      <td>0.093397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.298400</td>\n",
       "      <td>0.328609</td>\n",
       "      <td>0.880236</td>\n",
       "      <td>0.880236</td>\n",
       "      <td>0.760534</td>\n",
       "      <td>0.880263</td>\n",
       "      <td>0.880271</td>\n",
       "      <td>0.949852</td>\n",
       "      <td>0.950800</td>\n",
       "      <td>0.092448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.296800</td>\n",
       "      <td>0.334488</td>\n",
       "      <td>0.879730</td>\n",
       "      <td>0.879572</td>\n",
       "      <td>0.760669</td>\n",
       "      <td>0.881146</td>\n",
       "      <td>0.879526</td>\n",
       "      <td>0.950976</td>\n",
       "      <td>0.951868</td>\n",
       "      <td>0.092176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.317200</td>\n",
       "      <td>0.333878</td>\n",
       "      <td>0.882264</td>\n",
       "      <td>0.882195</td>\n",
       "      <td>0.764924</td>\n",
       "      <td>0.882784</td>\n",
       "      <td>0.882141</td>\n",
       "      <td>0.950605</td>\n",
       "      <td>0.951720</td>\n",
       "      <td>0.091955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.269000</td>\n",
       "      <td>0.331403</td>\n",
       "      <td>0.882601</td>\n",
       "      <td>0.882584</td>\n",
       "      <td>0.765217</td>\n",
       "      <td>0.882657</td>\n",
       "      <td>0.882561</td>\n",
       "      <td>0.950807</td>\n",
       "      <td>0.952291</td>\n",
       "      <td>0.091027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.251800</td>\n",
       "      <td>0.334263</td>\n",
       "      <td>0.883446</td>\n",
       "      <td>0.883365</td>\n",
       "      <td>0.767409</td>\n",
       "      <td>0.884102</td>\n",
       "      <td>0.883308</td>\n",
       "      <td>0.951923</td>\n",
       "      <td>0.953091</td>\n",
       "      <td>0.090701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.287800</td>\n",
       "      <td>0.337089</td>\n",
       "      <td>0.883615</td>\n",
       "      <td>0.883587</td>\n",
       "      <td>0.767306</td>\n",
       "      <td>0.883755</td>\n",
       "      <td>0.883551</td>\n",
       "      <td>0.950941</td>\n",
       "      <td>0.952377</td>\n",
       "      <td>0.092194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.259900</td>\n",
       "      <td>0.345972</td>\n",
       "      <td>0.880743</td>\n",
       "      <td>0.880718</td>\n",
       "      <td>0.762235</td>\n",
       "      <td>0.881353</td>\n",
       "      <td>0.880882</td>\n",
       "      <td>0.952271</td>\n",
       "      <td>0.953648</td>\n",
       "      <td>0.093371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>0.268800</td>\n",
       "      <td>0.340897</td>\n",
       "      <td>0.880743</td>\n",
       "      <td>0.880647</td>\n",
       "      <td>0.762119</td>\n",
       "      <td>0.881528</td>\n",
       "      <td>0.880592</td>\n",
       "      <td>0.951490</td>\n",
       "      <td>0.952591</td>\n",
       "      <td>0.092998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.318400</td>\n",
       "      <td>0.330503</td>\n",
       "      <td>0.885304</td>\n",
       "      <td>0.885126</td>\n",
       "      <td>0.772141</td>\n",
       "      <td>0.887065</td>\n",
       "      <td>0.885079</td>\n",
       "      <td>0.953367</td>\n",
       "      <td>0.954447</td>\n",
       "      <td>0.090468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>0.268600</td>\n",
       "      <td>0.341274</td>\n",
       "      <td>0.884966</td>\n",
       "      <td>0.884706</td>\n",
       "      <td>0.772363</td>\n",
       "      <td>0.887683</td>\n",
       "      <td>0.884686</td>\n",
       "      <td>0.953719</td>\n",
       "      <td>0.955009</td>\n",
       "      <td>0.091678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.266900</td>\n",
       "      <td>0.335428</td>\n",
       "      <td>0.886655</td>\n",
       "      <td>0.886653</td>\n",
       "      <td>0.773311</td>\n",
       "      <td>0.886649</td>\n",
       "      <td>0.886663</td>\n",
       "      <td>0.954041</td>\n",
       "      <td>0.955230</td>\n",
       "      <td>0.089843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>0.325100</td>\n",
       "      <td>0.315866</td>\n",
       "      <td>0.887669</td>\n",
       "      <td>0.887573</td>\n",
       "      <td>0.776044</td>\n",
       "      <td>0.888532</td>\n",
       "      <td>0.887512</td>\n",
       "      <td>0.954352</td>\n",
       "      <td>0.955627</td>\n",
       "      <td>0.087910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.254100</td>\n",
       "      <td>0.334973</td>\n",
       "      <td>0.885473</td>\n",
       "      <td>0.885241</td>\n",
       "      <td>0.773076</td>\n",
       "      <td>0.887871</td>\n",
       "      <td>0.885210</td>\n",
       "      <td>0.954414</td>\n",
       "      <td>0.955817</td>\n",
       "      <td>0.090274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>0.243100</td>\n",
       "      <td>0.318663</td>\n",
       "      <td>0.889358</td>\n",
       "      <td>0.889294</td>\n",
       "      <td>0.779126</td>\n",
       "      <td>0.889891</td>\n",
       "      <td>0.889235</td>\n",
       "      <td>0.954835</td>\n",
       "      <td>0.956262</td>\n",
       "      <td>0.087470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.282800</td>\n",
       "      <td>0.330401</td>\n",
       "      <td>0.887162</td>\n",
       "      <td>0.886916</td>\n",
       "      <td>0.776668</td>\n",
       "      <td>0.889786</td>\n",
       "      <td>0.886888</td>\n",
       "      <td>0.954685</td>\n",
       "      <td>0.956216</td>\n",
       "      <td>0.089346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18500</td>\n",
       "      <td>0.239100</td>\n",
       "      <td>0.336841</td>\n",
       "      <td>0.885473</td>\n",
       "      <td>0.885467</td>\n",
       "      <td>0.771274</td>\n",
       "      <td>0.885713</td>\n",
       "      <td>0.885561</td>\n",
       "      <td>0.955058</td>\n",
       "      <td>0.956425</td>\n",
       "      <td>0.089910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>0.268100</td>\n",
       "      <td>0.328030</td>\n",
       "      <td>0.890203</td>\n",
       "      <td>0.890112</td>\n",
       "      <td>0.781084</td>\n",
       "      <td>0.891036</td>\n",
       "      <td>0.890049</td>\n",
       "      <td>0.955789</td>\n",
       "      <td>0.956942</td>\n",
       "      <td>0.087608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19500</td>\n",
       "      <td>0.293600</td>\n",
       "      <td>0.325495</td>\n",
       "      <td>0.889358</td>\n",
       "      <td>0.889225</td>\n",
       "      <td>0.779836</td>\n",
       "      <td>0.890673</td>\n",
       "      <td>0.889165</td>\n",
       "      <td>0.956235</td>\n",
       "      <td>0.957292</td>\n",
       "      <td>0.087441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.264900</td>\n",
       "      <td>0.330149</td>\n",
       "      <td>0.887500</td>\n",
       "      <td>0.887394</td>\n",
       "      <td>0.775811</td>\n",
       "      <td>0.888479</td>\n",
       "      <td>0.887333</td>\n",
       "      <td>0.956273</td>\n",
       "      <td>0.957123</td>\n",
       "      <td>0.087823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20500</td>\n",
       "      <td>0.299900</td>\n",
       "      <td>0.329289</td>\n",
       "      <td>0.887838</td>\n",
       "      <td>0.887680</td>\n",
       "      <td>0.777044</td>\n",
       "      <td>0.889421</td>\n",
       "      <td>0.887625</td>\n",
       "      <td>0.956315</td>\n",
       "      <td>0.957212</td>\n",
       "      <td>0.087994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>0.330900</td>\n",
       "      <td>0.318204</td>\n",
       "      <td>0.891216</td>\n",
       "      <td>0.891157</td>\n",
       "      <td>0.782803</td>\n",
       "      <td>0.891704</td>\n",
       "      <td>0.891099</td>\n",
       "      <td>0.956576</td>\n",
       "      <td>0.957672</td>\n",
       "      <td>0.086585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21500</td>\n",
       "      <td>0.350400</td>\n",
       "      <td>0.312730</td>\n",
       "      <td>0.889696</td>\n",
       "      <td>0.889614</td>\n",
       "      <td>0.779973</td>\n",
       "      <td>0.890421</td>\n",
       "      <td>0.889553</td>\n",
       "      <td>0.956749</td>\n",
       "      <td>0.957847</td>\n",
       "      <td>0.085416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.287500</td>\n",
       "      <td>0.312789</td>\n",
       "      <td>0.889696</td>\n",
       "      <td>0.889640</td>\n",
       "      <td>0.779720</td>\n",
       "      <td>0.890136</td>\n",
       "      <td>0.889585</td>\n",
       "      <td>0.956612</td>\n",
       "      <td>0.957915</td>\n",
       "      <td>0.085806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22500</td>\n",
       "      <td>0.313900</td>\n",
       "      <td>0.312241</td>\n",
       "      <td>0.889696</td>\n",
       "      <td>0.889590</td>\n",
       "      <td>0.780221</td>\n",
       "      <td>0.890695</td>\n",
       "      <td>0.889528</td>\n",
       "      <td>0.957010</td>\n",
       "      <td>0.958225</td>\n",
       "      <td>0.085579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>0.268200</td>\n",
       "      <td>0.316245</td>\n",
       "      <td>0.891554</td>\n",
       "      <td>0.891529</td>\n",
       "      <td>0.783185</td>\n",
       "      <td>0.891693</td>\n",
       "      <td>0.891492</td>\n",
       "      <td>0.956983</td>\n",
       "      <td>0.958104</td>\n",
       "      <td>0.085953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23500</td>\n",
       "      <td>0.290900</td>\n",
       "      <td>0.318737</td>\n",
       "      <td>0.892230</td>\n",
       "      <td>0.892215</td>\n",
       "      <td>0.784475</td>\n",
       "      <td>0.892285</td>\n",
       "      <td>0.892191</td>\n",
       "      <td>0.957188</td>\n",
       "      <td>0.958191</td>\n",
       "      <td>0.085855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.262100</td>\n",
       "      <td>0.314161</td>\n",
       "      <td>0.891554</td>\n",
       "      <td>0.891552</td>\n",
       "      <td>0.783111</td>\n",
       "      <td>0.891548</td>\n",
       "      <td>0.891563</td>\n",
       "      <td>0.957084</td>\n",
       "      <td>0.958255</td>\n",
       "      <td>0.085860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24500</td>\n",
       "      <td>0.317900</td>\n",
       "      <td>0.310812</td>\n",
       "      <td>0.888345</td>\n",
       "      <td>0.888223</td>\n",
       "      <td>0.777668</td>\n",
       "      <td>0.889506</td>\n",
       "      <td>0.888163</td>\n",
       "      <td>0.956886</td>\n",
       "      <td>0.958066</td>\n",
       "      <td>0.086101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.230700</td>\n",
       "      <td>0.316042</td>\n",
       "      <td>0.889865</td>\n",
       "      <td>0.889681</td>\n",
       "      <td>0.781442</td>\n",
       "      <td>0.891815</td>\n",
       "      <td>0.889629</td>\n",
       "      <td>0.957326</td>\n",
       "      <td>0.958435</td>\n",
       "      <td>0.086810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25500</td>\n",
       "      <td>0.304900</td>\n",
       "      <td>0.323792</td>\n",
       "      <td>0.889865</td>\n",
       "      <td>0.889865</td>\n",
       "      <td>0.779795</td>\n",
       "      <td>0.889895</td>\n",
       "      <td>0.889901</td>\n",
       "      <td>0.957119</td>\n",
       "      <td>0.958178</td>\n",
       "      <td>0.086905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.220400</td>\n",
       "      <td>0.326353</td>\n",
       "      <td>0.892399</td>\n",
       "      <td>0.892394</td>\n",
       "      <td>0.784789</td>\n",
       "      <td>0.892392</td>\n",
       "      <td>0.892397</td>\n",
       "      <td>0.957327</td>\n",
       "      <td>0.958384</td>\n",
       "      <td>0.086651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26500</td>\n",
       "      <td>0.242700</td>\n",
       "      <td>0.321492</td>\n",
       "      <td>0.893412</td>\n",
       "      <td>0.893381</td>\n",
       "      <td>0.786952</td>\n",
       "      <td>0.893614</td>\n",
       "      <td>0.893338</td>\n",
       "      <td>0.957459</td>\n",
       "      <td>0.958548</td>\n",
       "      <td>0.086129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>0.263200</td>\n",
       "      <td>0.320977</td>\n",
       "      <td>0.893074</td>\n",
       "      <td>0.893064</td>\n",
       "      <td>0.786146</td>\n",
       "      <td>0.893098</td>\n",
       "      <td>0.893048</td>\n",
       "      <td>0.957474</td>\n",
       "      <td>0.958575</td>\n",
       "      <td>0.086419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27500</td>\n",
       "      <td>0.245700</td>\n",
       "      <td>0.317444</td>\n",
       "      <td>0.894426</td>\n",
       "      <td>0.894384</td>\n",
       "      <td>0.789072</td>\n",
       "      <td>0.894739</td>\n",
       "      <td>0.894333</td>\n",
       "      <td>0.957710</td>\n",
       "      <td>0.958756</td>\n",
       "      <td>0.085554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.257100</td>\n",
       "      <td>0.318174</td>\n",
       "      <td>0.892905</td>\n",
       "      <td>0.892844</td>\n",
       "      <td>0.786218</td>\n",
       "      <td>0.893435</td>\n",
       "      <td>0.892784</td>\n",
       "      <td>0.957791</td>\n",
       "      <td>0.958794</td>\n",
       "      <td>0.085559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28500</td>\n",
       "      <td>0.267700</td>\n",
       "      <td>0.318808</td>\n",
       "      <td>0.893074</td>\n",
       "      <td>0.893003</td>\n",
       "      <td>0.786657</td>\n",
       "      <td>0.893717</td>\n",
       "      <td>0.892940</td>\n",
       "      <td>0.957815</td>\n",
       "      <td>0.958859</td>\n",
       "      <td>0.085747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>0.277700</td>\n",
       "      <td>0.318853</td>\n",
       "      <td>0.894088</td>\n",
       "      <td>0.894042</td>\n",
       "      <td>0.788436</td>\n",
       "      <td>0.894448</td>\n",
       "      <td>0.893988</td>\n",
       "      <td>0.957837</td>\n",
       "      <td>0.958875</td>\n",
       "      <td>0.085649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29500</td>\n",
       "      <td>0.250500</td>\n",
       "      <td>0.318815</td>\n",
       "      <td>0.893919</td>\n",
       "      <td>0.893875</td>\n",
       "      <td>0.788077</td>\n",
       "      <td>0.894255</td>\n",
       "      <td>0.893823</td>\n",
       "      <td>0.957850</td>\n",
       "      <td>0.958882</td>\n",
       "      <td>0.085639</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/common/zhangz2lab/zhanh/output_budget_0918/results/run\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='370' max='370' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [370/370 00:18]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2b9170",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61853275",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94662245",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279a0e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be16700",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718f3f78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DNA Peft",
   "language": "python",
   "name": "dna_peft"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
