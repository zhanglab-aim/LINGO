{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c364732",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bf01f08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/common/zhanh/anaconda3/envs/dna_lora/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import csv\n",
    "import copy\n",
    "import json\n",
    "import logging\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional, Dict, Sequence, Tuple, List\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "import sklearn\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, brier_score_loss\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from scipy.special import softmax\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    AdaLoraConfig,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    "    AdaLoraModel\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cb03741",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainerCallback, TrainerControl, TrainerState, TrainingArguments\n",
    "\n",
    "class EvalAndSaveCallback(TrainerCallback):\n",
    "    def __init__(self, trainer):\n",
    "        self.trainer = trainer\n",
    "        \n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        if state.global_step % 500 == 0:\n",
    "            results = self.trainer.evaluate()\n",
    "            results_file = os.path.join(args.output_dir, f\"results_step_{state.global_step}.json\")\n",
    "            with open(results_file, \"w\") as f:\n",
    "                json.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec7ea068",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelArguments:\n",
    "    #model_name_or_path: Optional[str] = field(default=\"zhihan1996/DNABERT-2-117M\")\n",
    "    model_name_or_path: Optional[str] = field(default=\"facebook/opt-125m\")\n",
    "    #model_name_or_path: Optional[str] = field(default=\"decapoda-research/llama-7b-hf\")\n",
    "    use_lora: bool = field(default=True, metadata={\"help\": \"whether to use LoRA\"})\n",
    "    lora_r: int = field(default=8, metadata={\"help\": \"hidden dimension for LoRA\"})\n",
    "    lora_alpha: int = field(default=32, metadata={\"help\": \"alpha for LoRA\"})\n",
    "    lora_dropout: float = field(default=0.05, metadata={\"help\": \"dropout rate for LoRA\"})\n",
    "    lora_target_modules: str = field(default=\"k_proj,q_proj,v_proj,fc1,fc2,output_proj\", metadata={\"help\": \"where to perform LoRA\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "253335f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataArguments:\n",
    "    data_path: str = field(default=\"/common/zhanh/DNABERT_2/GUE/EMP/H3\", metadata={\"help\": \"Path to the training data.\"})\n",
    "    kmer: int = field(default=-1, metadata={\"help\": \"k-mer for input sequence. -1 means not using k-mer.\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "871af735",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "@dataclass\n",
    "class TrainingArguments(transformers.TrainingArguments):\n",
    "    cache_dir: Optional[str] = field(default=None)\n",
    "    run_name: str = field(default=\"run\")\n",
    "    optim: str = field(default=\"adamw_torch\")\n",
    "    model_max_length: int = field(default=512, metadata={\"help\": \"Maximum sequence length.\"})\n",
    "    gradient_accumulation_steps: int = field(default=1)\n",
    "    per_device_train_batch_size: int = field(default=8)\n",
    "    per_device_eval_batch_size: int = field(default=16)\n",
    "    num_train_epochs: int = field(default=100)\n",
    "    fp16: bool = field(default=False)\n",
    "    logging_steps: int = field(default=1000)\n",
    "    save_steps: int = field(default=500)\n",
    "    eval_steps: int = field(default=500)\n",
    "    evaluation_strategy: str = field(default=\"steps\")\n",
    "    load_best_model_at_end: bool = field(default=True)     # load the best model when finished training (default metric is loss)\n",
    "    metric_for_best_model: str = field(default=\"matthews_correlation\") # the metric to use to compare models\n",
    "    greater_is_better: bool = field(default=True)           # whether the `metric_for_best_model` should be maximized or not\n",
    "    logging_strategy: str = field(default=\"steps\")  # Log every \"steps\"\n",
    "    logging_steps: int = field(default=100)  # Log every 100 steps\n",
    "    warmup_steps: int = field(default=50)\n",
    "    weight_decay: float = field(default=0.01)\n",
    "    learning_rate: float = field(default=1e-4)\n",
    "    save_total_limit: int = field(default=10)\n",
    "    load_best_model_at_end: bool = field(default=True)\n",
    "    output_dir: str = field(default=\"/common/zhanh/DNABERT_2/output\")\n",
    "    find_unused_parameters: bool = field(default=False)\n",
    "    checkpointing: bool = field(default=False)\n",
    "    dataloader_pin_memory: bool = field(default=False)\n",
    "    eval_and_save_results: bool = field(default=True)\n",
    "    save_model: bool = field(default=False)\n",
    "    seed: int = field(default=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68d3a56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_save_model_for_hf_trainer(trainer: transformers.Trainer, output_dir: str):\n",
    "    \"\"\"Collects the state dict and dump to disk.\"\"\"\n",
    "    #state_dict = trainer.model.state_dict()\n",
    "    if trainer.args.should_save:\n",
    "        #cpu_state_dict = {key: value.cpu() for key, value in state_dict.items()}\n",
    "        #del state_dict\n",
    "        #trainer._save(output_dir, state_dict=cpu_state_dict)  # noqa\n",
    "        checkpoint_dir = os.path.join(output_dir, f\"checkpoint-{trainer.state.global_step}\")\n",
    "        trainer.model.save_pretrained(checkpoint_dir)\n",
    "        trainer.model.config.save_pretrained(checkpoint_dir) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8fd211c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_alter_of_dna_sequence(sequence: str):\n",
    "    MAP = {\"A\": \"T\", \"T\": \"A\", \"C\": \"G\", \"G\": \"C\"}\n",
    "    # return \"\".join([MAP[c] for c in reversed(sequence)])\n",
    "    return \"\".join([MAP[c] for c in sequence])\n",
    "\n",
    "\"\"\"\n",
    "Transform a dna sequence to k-mer string\n",
    "\"\"\"\n",
    "def generate_kmer_str(sequence: str, k: int) -> str:\n",
    "    \"\"\"Generate k-mer string from DNA sequence.\"\"\"\n",
    "    return \" \".join([sequence[i:i+k] for i in range(len(sequence) - k + 1)])\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Load or generate k-mer string for each DNA sequence. The generated k-mer string will be saved to the same directory as the original data with the same name but with a suffix of \"_{k}mer\".\n",
    "\"\"\"\n",
    "def load_or_generate_kmer(data_path: str, texts: List[str], k: int) -> List[str]:\n",
    "    \"\"\"Load or generate k-mer string for each DNA sequence.\"\"\"\n",
    "    kmer_path = data_path.replace(\".csv\", f\"_{k}mer.json\")\n",
    "    if os.path.exists(kmer_path):\n",
    "        logging.warning(f\"Loading k-mer from {kmer_path}...\")\n",
    "        with open(kmer_path, \"r\") as f:\n",
    "            kmer = json.load(f)\n",
    "    else:        \n",
    "        logging.warning(f\"Generating k-mer...\")\n",
    "        kmer = [generate_kmer_str(text, k) for text in texts]\n",
    "        with open(kmer_path, \"w\") as f:\n",
    "            logging.warning(f\"Saving k-mer to {kmer_path}...\")\n",
    "            json.dump(kmer, f)\n",
    "        \n",
    "    return kmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7cc491b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SupervisedDataset(Dataset):\n",
    "    \"\"\"Dataset for supervised fine-tuning.\"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 data_path: str, \n",
    "                 tokenizer: transformers.PreTrainedTokenizer, \n",
    "                 kmer: int = -1):\n",
    "\n",
    "        super(SupervisedDataset, self).__init__()\n",
    "\n",
    "        # load data from the disk\n",
    "        with open(data_path, \"r\") as f:\n",
    "            data = list(csv.reader(f))[1:]\n",
    "        if len(data[0]) == 2:\n",
    "            # data is in the format of [text, label]\n",
    "            logging.warning(\"Perform single sequence classification...\")\n",
    "            texts = [d[0] for d in data]\n",
    "            labels = [int(d[1]) for d in data]\n",
    "        elif len(data[0]) == 3:\n",
    "            # data is in the format of [text1, text2, label]\n",
    "            logging.warning(\"Perform sequence-pair classification...\")\n",
    "            texts = [[d[0], d[1]] for d in data]\n",
    "            labels = [int(d[2]) for d in data]\n",
    "        else:\n",
    "            raise ValueError(\"Data format not supported.\")\n",
    "        \n",
    "        if kmer != -1:\n",
    "            # only write file on the first process\n",
    "            if torch.distributed.get_rank() not in [0, -1]:\n",
    "                torch.distributed.barrier()\n",
    "\n",
    "            logging.warning(f\"Using {kmer}-mer as input...\")\n",
    "            texts = load_or_generate_kmer(data_path, texts, kmer)\n",
    "\n",
    "            if torch.distributed.get_rank() == 0:\n",
    "                torch.distributed.barrier()\n",
    "\n",
    "        output = tokenizer(\n",
    "            texts,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"longest\",\n",
    "            max_length=tokenizer.model_max_length,\n",
    "            truncation=True,\n",
    "        )\n",
    "\n",
    "        self.input_ids = output[\"input_ids\"]\n",
    "        self.attention_mask = output[\"attention_mask\"]\n",
    "        self.labels = labels\n",
    "        self.num_labels = len(set(labels))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n",
    "        return dict(input_ids=self.input_ids[i], labels=self.labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c12ffb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollatorForSupervisedDataset(object):\n",
    "    \"\"\"Collate examples for supervised fine-tuning.\"\"\"\n",
    "\n",
    "    tokenizer: transformers.PreTrainedTokenizer\n",
    "\n",
    "    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:\n",
    "        input_ids, labels = tuple([instance[key] for instance in instances] for key in (\"input_ids\", \"labels\"))\n",
    "        input_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "            input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id\n",
    "        )\n",
    "        labels = torch.Tensor(labels).long()\n",
    "        return dict(\n",
    "            input_ids=input_ids,\n",
    "            labels=labels,\n",
    "            attention_mask=input_ids.ne(self.tokenizer.pad_token_id),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e0f1ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metric_with_sklearn(logits: np.ndarray, labels: np.ndarray):\n",
    "    if logits.ndim == 3:\n",
    "        # Reshape logits to 2D if needed\n",
    "        logits = logits.reshape(-1, logits.shape[-1])\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    valid_mask = labels != -100  # Exclude padding tokens (assuming -100 is the padding token ID)\n",
    "    valid_predictions = predictions[valid_mask]\n",
    "    valid_labels = labels[valid_mask]\n",
    "    # Compute probabilities from logits\n",
    "    probabilities = softmax(logits, axis=-1)\n",
    "\n",
    "    # Extract the probabilities corresponding to the positive class\n",
    "    valid_scores = probabilities[valid_mask, 1]  # assuming the second column is the positive class\n",
    "    return {\n",
    "        \"accuracy\": sklearn.metrics.accuracy_score(valid_labels, valid_predictions),\n",
    "        \"f1\": sklearn.metrics.f1_score(\n",
    "            valid_labels, valid_predictions, average=\"macro\", zero_division=0\n",
    "        ),\n",
    "        \"matthews_correlation\": sklearn.metrics.matthews_corrcoef(\n",
    "            valid_labels, valid_predictions\n",
    "        ),\n",
    "        \"precision\": sklearn.metrics.precision_score(\n",
    "            valid_labels, valid_predictions, average=\"macro\", zero_division=0\n",
    "        ),\n",
    "        \"recall\": sklearn.metrics.recall_score(\n",
    "            valid_labels, valid_predictions, average=\"macro\", zero_division=0\n",
    "        ),\n",
    "        \"pr_auc\": average_precision_score(valid_labels, valid_scores),\n",
    "        \"roc_auc\": roc_auc_score(valid_labels, valid_scores),\n",
    "        \"brier_score\": brier_score_loss(valid_labels, valid_scores)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b1b13d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Compute metrics used for huggingface trainer.\n",
    "\"\"\" \n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    if isinstance(logits, tuple):  # Unpack logits if it's a tuple\n",
    "        logits = logits[0]\n",
    "    return calculate_metric_with_sklearn(logits, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "55a32efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    #parser = transformers.HfArgumentParser((ModelArguments, DataArguments, TrainingArguments))\n",
    "    #print(parser)\n",
    "    #model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n",
    "    parser = transformers.HfArgumentParser((ModelArguments, DataArguments, TrainingArguments,))\n",
    "    model_args, data_args, training_args, remaining = parser.parse_args_into_dataclasses(return_remaining_strings=True)\n",
    "\n",
    "    # load tokenizer\n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "        model_args.model_name_or_path,\n",
    "        cache_dir=training_args.cache_dir,\n",
    "        model_max_length=training_args.model_max_length,\n",
    "        padding_side=\"right\",\n",
    "        use_fast=True,\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    \n",
    "\n",
    "    if \"InstaDeepAI\" in model_args.model_name_or_path:\n",
    "        tokenizer.eos_token = tokenizer.pad_token\n",
    "\n",
    "    # define datasets and data collator\n",
    "    train_dataset = SupervisedDataset(tokenizer=tokenizer, \n",
    "                                      data_path=os.path.join(data_args.data_path, \"train.csv\"), \n",
    "                                      kmer=data_args.kmer)\n",
    "    val_dataset = SupervisedDataset(tokenizer=tokenizer, \n",
    "                                     data_path=os.path.join(data_args.data_path, \"dev.csv\"), \n",
    "                                     kmer=data_args.kmer)\n",
    "    test_dataset = SupervisedDataset(tokenizer=tokenizer, \n",
    "                                     data_path=os.path.join(data_args.data_path, \"test.csv\"), \n",
    "                                     kmer=data_args.kmer)\n",
    "    data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)\n",
    "\n",
    "    #config = transformers.AutoConfig.from_pretrained(model_args.model_name_or_path, trust_remote_code=True)\n",
    "    # load model\n",
    "    model = transformers.AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_args.model_name_or_path,\n",
    "        #config = config,\n",
    "        cache_dir=training_args.cache_dir,\n",
    "        num_labels=train_dataset.num_labels,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    #module_names = [name for name, _ in model.named_modules()]\n",
    "    #print([(n, type(m)) for n, m in model.named_modules()])\n",
    "    module_names_and_types = [(n, type(m)) for n, m in model.named_modules()]\n",
    "    module = \",\".join(n for n, _ in module_names_and_types)\n",
    "    target = list(module.split(\",\"))\n",
    "    print(module)\n",
    "    # Print the list\n",
    "    #print(module_names)\n",
    "    \n",
    "    # Get the names of the layers in the base_model\n",
    "\n",
    "    #for layer_idx, layer in enumerate(model.base_model.encoder.layer):\n",
    "        #print(f\"Layer {layer_idx}:\")\n",
    "    \n",
    "        # Get the self-attention layer\n",
    "        #self_attention_layer = layer.attention.self\n",
    "    \n",
    "        # Print the names of the sub-components\n",
    "        #for name, module in self_attention_layer.named_children():\n",
    "            #print(f\"  {name}\")\n",
    "\n",
    "    # configure LoRA\n",
    "    #model_args.lora_target_modules = r\"bert\\.encoder\\.layer\\.\\d+\\.mlp\\.wo\" \n",
    "    if model_args.use_lora:\n",
    "        lora_config = AdaLoraConfig(\n",
    "            r = model_args.lora_r,\n",
    "            init_r = 12,\n",
    "            target_r = 8,\n",
    "            #target_modules=list(r\"bert\\.encoder\\.layer\\.\\d+\\.mlp\\.wo\"),\n",
    "            lora_alpha=model_args.lora_alpha,\n",
    "            target_modules=list(model_args.lora_target_modules.split(\",\")),\n",
    "            #target_modules = target[1:],\n",
    "            lora_dropout=model_args.lora_dropout,\n",
    "            bias=\"none\",\n",
    "            task_type=\"SEQ_CLS\",\n",
    "            inference_mode=False,\n",
    "            peft_type=\"ADALORA\",\n",
    "        )\n",
    "        print(list(model_args.lora_target_modules.split(\",\")))\n",
    "        model = get_peft_model(model, lora_config)\n",
    "        #model = AdaLoraModel(model, lora_config, \"default\")\n",
    "        model.print_trainable_parameters()\n",
    "\n",
    "\n",
    "    # define trainer\n",
    "    trainer = transformers.Trainer(model=model,\n",
    "                                   tokenizer=tokenizer,\n",
    "                                   args=training_args,\n",
    "                                   compute_metrics=compute_metrics,\n",
    "                                   train_dataset=train_dataset,\n",
    "                                   eval_dataset=test_dataset,\n",
    "                                   data_collator=data_collator,\n",
    "                                  )\n",
    "\n",
    "    callback = EvalAndSaveCallback(trainer)\n",
    "    trainer.add_callback(callback)\n",
    "    trainer.train()\n",
    "\n",
    "    if training_args.save_model:\n",
    "        trainer.save_state()\n",
    "        safe_save_model_for_hf_trainer(trainer=trainer, output_dir=training_args.output_dir)\n",
    "\n",
    "    # get the evaluation results from trainer\n",
    "    if training_args.eval_and_save_results:\n",
    "        results_path = os.path.join(training_args.output_dir, \"results\", training_args.run_name)\n",
    "        print(results_path)\n",
    "        results = trainer.evaluate(eval_dataset=test_dataset)\n",
    "        os.makedirs(results_path, exist_ok=True)\n",
    "        with open(os.path.join(results_path, \"eval_results.json\"), \"w\") as f:\n",
    "            json.dump(results, f)\n",
    "    \n",
    "    # Get all checkpoint directories\n",
    "#     if training_args.eval_and_save_results:\n",
    "#     # Get list of all checkpoints\n",
    "#         checkpoints = [dir_name for dir_name in os.listdir(training_args.output_dir) \n",
    "#                    if 'checkpoint' in dir_name]\n",
    "    \n",
    "#         for checkpoint in checkpoints:\n",
    "#             checkpoint_path = os.path.join(training_args.output_dir, checkpoint)\n",
    "        \n",
    "#         # Load model from checkpoint\n",
    "#             model_checkpoint = transformers.AutoModelForSequenceClassification.from_pretrained(checkpoint_path,trust_remote_code=True)\n",
    "#             trainer.model = model_checkpoint  # Update trainer's model\n",
    "#             print(model.device)\n",
    "#             print(next(model.parameters()).device)\n",
    "#             for key, value in inputs.items():\n",
    "#                 if isinstance(value, torch.Tensor):\n",
    "#                     print(f\"{key} is on {value.device}\")\n",
    "#             #print(labels.device)\n",
    "#             device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#             input_ids = input_ids.to(device)\n",
    "#             token_type_ids = token_type_ids.to(device)\n",
    "#             attention_mask = attention_mask.to(device)\n",
    "\n",
    "#             results = trainer.evaluate(eval_dataset=test_dataset)\n",
    "        \n",
    "#         # Modify results_path to include checkpoint name\n",
    "#             results_path = os.path.join(training_args.output_dir, \"results\", checkpoint)\n",
    "#             os.makedirs(results_path, exist_ok=True)\n",
    "        \n",
    "#             with open(os.path.join(results_path, \"eval_results.json\"), \"w\") as f:\n",
    "#                 json.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a537eeab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "cache_dir=None,\n",
      "checkpointing=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=False,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_and_save_results=True,\n",
      "eval_delay=0,\n",
      "eval_steps=500,\n",
      "evaluation_strategy=steps,\n",
      "find_unused_parameters=False,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=True,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=0.0001,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=True,\n",
      "local_rank=-1,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=/common/zhanh/DNABERT_2/output/runs/Aug09_13-14-20_esplhpc-cp055,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=100,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=matthews_correlation,\n",
      "model_max_length=512,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=100,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "output_dir=/common/zhanh/DNABERT_2/output,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=16,\n",
      "per_device_train_batch_size=8,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=run,\n",
      "save_model=False,\n",
      "save_on_each_node=False,\n",
      "save_safetensors=False,\n",
      "save_steps=500,\n",
      "save_strategy=steps,\n",
      "save_total_limit=10,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=50,\n",
      "weight_decay=0.01,\n",
      "xpu_backend=None,\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#parser = transformers.HfArgumentParser((ModelArguments, DataArguments, TrainingArguments))\n",
    "#parser.parse_args_into_dataclasses()\n",
    "\n",
    "parser = transformers.HfArgumentParser((ModelArguments, DataArguments, TrainingArguments,))\n",
    "model_args, data_args, training_args, remaining = parser.parse_args_into_dataclasses(return_remaining_strings=True)\n",
    "print(training_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7717f87f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/opt-125m were not used when initializing OPTForSequenceClassification: ['lm_head.weight']\n",
      "- This IS expected if you are initializing OPTForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing OPTForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of OPTForSequenceClassification were not initialized from the model checkpoint at facebook/opt-125m and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ",model,model.decoder,model.decoder.embed_tokens,model.decoder.embed_positions,model.decoder.final_layer_norm,model.decoder.layers,model.decoder.layers.0,model.decoder.layers.0.self_attn,model.decoder.layers.0.self_attn.k_proj,model.decoder.layers.0.self_attn.v_proj,model.decoder.layers.0.self_attn.q_proj,model.decoder.layers.0.self_attn.out_proj,model.decoder.layers.0.activation_fn,model.decoder.layers.0.self_attn_layer_norm,model.decoder.layers.0.fc1,model.decoder.layers.0.fc2,model.decoder.layers.0.final_layer_norm,model.decoder.layers.1,model.decoder.layers.1.self_attn,model.decoder.layers.1.self_attn.k_proj,model.decoder.layers.1.self_attn.v_proj,model.decoder.layers.1.self_attn.q_proj,model.decoder.layers.1.self_attn.out_proj,model.decoder.layers.1.activation_fn,model.decoder.layers.1.self_attn_layer_norm,model.decoder.layers.1.fc1,model.decoder.layers.1.fc2,model.decoder.layers.1.final_layer_norm,model.decoder.layers.2,model.decoder.layers.2.self_attn,model.decoder.layers.2.self_attn.k_proj,model.decoder.layers.2.self_attn.v_proj,model.decoder.layers.2.self_attn.q_proj,model.decoder.layers.2.self_attn.out_proj,model.decoder.layers.2.activation_fn,model.decoder.layers.2.self_attn_layer_norm,model.decoder.layers.2.fc1,model.decoder.layers.2.fc2,model.decoder.layers.2.final_layer_norm,model.decoder.layers.3,model.decoder.layers.3.self_attn,model.decoder.layers.3.self_attn.k_proj,model.decoder.layers.3.self_attn.v_proj,model.decoder.layers.3.self_attn.q_proj,model.decoder.layers.3.self_attn.out_proj,model.decoder.layers.3.activation_fn,model.decoder.layers.3.self_attn_layer_norm,model.decoder.layers.3.fc1,model.decoder.layers.3.fc2,model.decoder.layers.3.final_layer_norm,model.decoder.layers.4,model.decoder.layers.4.self_attn,model.decoder.layers.4.self_attn.k_proj,model.decoder.layers.4.self_attn.v_proj,model.decoder.layers.4.self_attn.q_proj,model.decoder.layers.4.self_attn.out_proj,model.decoder.layers.4.activation_fn,model.decoder.layers.4.self_attn_layer_norm,model.decoder.layers.4.fc1,model.decoder.layers.4.fc2,model.decoder.layers.4.final_layer_norm,model.decoder.layers.5,model.decoder.layers.5.self_attn,model.decoder.layers.5.self_attn.k_proj,model.decoder.layers.5.self_attn.v_proj,model.decoder.layers.5.self_attn.q_proj,model.decoder.layers.5.self_attn.out_proj,model.decoder.layers.5.activation_fn,model.decoder.layers.5.self_attn_layer_norm,model.decoder.layers.5.fc1,model.decoder.layers.5.fc2,model.decoder.layers.5.final_layer_norm,model.decoder.layers.6,model.decoder.layers.6.self_attn,model.decoder.layers.6.self_attn.k_proj,model.decoder.layers.6.self_attn.v_proj,model.decoder.layers.6.self_attn.q_proj,model.decoder.layers.6.self_attn.out_proj,model.decoder.layers.6.activation_fn,model.decoder.layers.6.self_attn_layer_norm,model.decoder.layers.6.fc1,model.decoder.layers.6.fc2,model.decoder.layers.6.final_layer_norm,model.decoder.layers.7,model.decoder.layers.7.self_attn,model.decoder.layers.7.self_attn.k_proj,model.decoder.layers.7.self_attn.v_proj,model.decoder.layers.7.self_attn.q_proj,model.decoder.layers.7.self_attn.out_proj,model.decoder.layers.7.activation_fn,model.decoder.layers.7.self_attn_layer_norm,model.decoder.layers.7.fc1,model.decoder.layers.7.fc2,model.decoder.layers.7.final_layer_norm,model.decoder.layers.8,model.decoder.layers.8.self_attn,model.decoder.layers.8.self_attn.k_proj,model.decoder.layers.8.self_attn.v_proj,model.decoder.layers.8.self_attn.q_proj,model.decoder.layers.8.self_attn.out_proj,model.decoder.layers.8.activation_fn,model.decoder.layers.8.self_attn_layer_norm,model.decoder.layers.8.fc1,model.decoder.layers.8.fc2,model.decoder.layers.8.final_layer_norm,model.decoder.layers.9,model.decoder.layers.9.self_attn,model.decoder.layers.9.self_attn.k_proj,model.decoder.layers.9.self_attn.v_proj,model.decoder.layers.9.self_attn.q_proj,model.decoder.layers.9.self_attn.out_proj,model.decoder.layers.9.activation_fn,model.decoder.layers.9.self_attn_layer_norm,model.decoder.layers.9.fc1,model.decoder.layers.9.fc2,model.decoder.layers.9.final_layer_norm,model.decoder.layers.10,model.decoder.layers.10.self_attn,model.decoder.layers.10.self_attn.k_proj,model.decoder.layers.10.self_attn.v_proj,model.decoder.layers.10.self_attn.q_proj,model.decoder.layers.10.self_attn.out_proj,model.decoder.layers.10.activation_fn,model.decoder.layers.10.self_attn_layer_norm,model.decoder.layers.10.fc1,model.decoder.layers.10.fc2,model.decoder.layers.10.final_layer_norm,model.decoder.layers.11,model.decoder.layers.11.self_attn,model.decoder.layers.11.self_attn.k_proj,model.decoder.layers.11.self_attn.v_proj,model.decoder.layers.11.self_attn.q_proj,model.decoder.layers.11.self_attn.out_proj,model.decoder.layers.11.activation_fn,model.decoder.layers.11.self_attn_layer_norm,model.decoder.layers.11.fc1,model.decoder.layers.11.fc2,model.decoder.layers.11.final_layer_norm,score\n",
      "['k_proj', 'q_proj', 'v_proj', 'fc1', 'fc2', 'output_proj']\n",
      "trainable params: 1,773,264 || all params: 127,012,620 || trainable%: 1.396132132381806\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='40565' max='149700' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 40565/149700 1:24:26 < 3:47:12, 8.01 it/s, Epoch 27.10/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Matthews Correlation</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Pr Auc</th>\n",
       "      <th>Roc Auc</th>\n",
       "      <th>Brier Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.665800</td>\n",
       "      <td>0.647823</td>\n",
       "      <td>0.619238</td>\n",
       "      <td>0.591969</td>\n",
       "      <td>0.285721</td>\n",
       "      <td>0.667935</td>\n",
       "      <td>0.621530</td>\n",
       "      <td>0.693373</td>\n",
       "      <td>0.726874</td>\n",
       "      <td>0.227800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.520200</td>\n",
       "      <td>0.519815</td>\n",
       "      <td>0.770207</td>\n",
       "      <td>0.770199</td>\n",
       "      <td>0.540409</td>\n",
       "      <td>0.770196</td>\n",
       "      <td>0.770213</td>\n",
       "      <td>0.796378</td>\n",
       "      <td>0.834971</td>\n",
       "      <td>0.165483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.484000</td>\n",
       "      <td>0.500337</td>\n",
       "      <td>0.780895</td>\n",
       "      <td>0.780531</td>\n",
       "      <td>0.564896</td>\n",
       "      <td>0.783593</td>\n",
       "      <td>0.781308</td>\n",
       "      <td>0.824689</td>\n",
       "      <td>0.857746</td>\n",
       "      <td>0.157531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.493900</td>\n",
       "      <td>0.530781</td>\n",
       "      <td>0.784903</td>\n",
       "      <td>0.781494</td>\n",
       "      <td>0.584998</td>\n",
       "      <td>0.801382</td>\n",
       "      <td>0.783877</td>\n",
       "      <td>0.839671</td>\n",
       "      <td>0.862364</td>\n",
       "      <td>0.163225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.447900</td>\n",
       "      <td>0.485007</td>\n",
       "      <td>0.798263</td>\n",
       "      <td>0.798029</td>\n",
       "      <td>0.597089</td>\n",
       "      <td>0.799062</td>\n",
       "      <td>0.798028</td>\n",
       "      <td>0.847495</td>\n",
       "      <td>0.869977</td>\n",
       "      <td>0.148154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.409000</td>\n",
       "      <td>0.476943</td>\n",
       "      <td>0.800267</td>\n",
       "      <td>0.797959</td>\n",
       "      <td>0.611779</td>\n",
       "      <td>0.812520</td>\n",
       "      <td>0.799399</td>\n",
       "      <td>0.868100</td>\n",
       "      <td>0.880704</td>\n",
       "      <td>0.148059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.458300</td>\n",
       "      <td>0.461765</td>\n",
       "      <td>0.820307</td>\n",
       "      <td>0.820275</td>\n",
       "      <td>0.641348</td>\n",
       "      <td>0.820863</td>\n",
       "      <td>0.820485</td>\n",
       "      <td>0.875712</td>\n",
       "      <td>0.887551</td>\n",
       "      <td>0.136694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.395800</td>\n",
       "      <td>0.436740</td>\n",
       "      <td>0.821643</td>\n",
       "      <td>0.821516</td>\n",
       "      <td>0.645096</td>\n",
       "      <td>0.823160</td>\n",
       "      <td>0.821937</td>\n",
       "      <td>0.879311</td>\n",
       "      <td>0.892314</td>\n",
       "      <td>0.132697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.467400</td>\n",
       "      <td>0.434413</td>\n",
       "      <td>0.822311</td>\n",
       "      <td>0.822276</td>\n",
       "      <td>0.644600</td>\n",
       "      <td>0.822348</td>\n",
       "      <td>0.822251</td>\n",
       "      <td>0.872812</td>\n",
       "      <td>0.891280</td>\n",
       "      <td>0.129148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.411600</td>\n",
       "      <td>0.411960</td>\n",
       "      <td>0.839011</td>\n",
       "      <td>0.838717</td>\n",
       "      <td>0.679389</td>\n",
       "      <td>0.840688</td>\n",
       "      <td>0.838703</td>\n",
       "      <td>0.886755</td>\n",
       "      <td>0.900816</td>\n",
       "      <td>0.124485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.388400</td>\n",
       "      <td>0.539237</td>\n",
       "      <td>0.793587</td>\n",
       "      <td>0.789912</td>\n",
       "      <td>0.612887</td>\n",
       "      <td>0.818552</td>\n",
       "      <td>0.794796</td>\n",
       "      <td>0.892316</td>\n",
       "      <td>0.907335</td>\n",
       "      <td>0.156038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.398800</td>\n",
       "      <td>0.427102</td>\n",
       "      <td>0.839011</td>\n",
       "      <td>0.838908</td>\n",
       "      <td>0.678284</td>\n",
       "      <td>0.839430</td>\n",
       "      <td>0.838854</td>\n",
       "      <td>0.882202</td>\n",
       "      <td>0.898900</td>\n",
       "      <td>0.125969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.380600</td>\n",
       "      <td>0.416113</td>\n",
       "      <td>0.838343</td>\n",
       "      <td>0.838156</td>\n",
       "      <td>0.677394</td>\n",
       "      <td>0.839284</td>\n",
       "      <td>0.838111</td>\n",
       "      <td>0.892808</td>\n",
       "      <td>0.906714</td>\n",
       "      <td>0.121210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.395600</td>\n",
       "      <td>0.442349</td>\n",
       "      <td>0.818303</td>\n",
       "      <td>0.817939</td>\n",
       "      <td>0.640579</td>\n",
       "      <td>0.821833</td>\n",
       "      <td>0.818754</td>\n",
       "      <td>0.886230</td>\n",
       "      <td>0.899347</td>\n",
       "      <td>0.134215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.349900</td>\n",
       "      <td>0.413011</td>\n",
       "      <td>0.839679</td>\n",
       "      <td>0.839507</td>\n",
       "      <td>0.679988</td>\n",
       "      <td>0.840531</td>\n",
       "      <td>0.839458</td>\n",
       "      <td>0.902894</td>\n",
       "      <td>0.912483</td>\n",
       "      <td>0.118431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.345500</td>\n",
       "      <td>0.426181</td>\n",
       "      <td>0.842351</td>\n",
       "      <td>0.842307</td>\n",
       "      <td>0.684723</td>\n",
       "      <td>0.842453</td>\n",
       "      <td>0.842270</td>\n",
       "      <td>0.904474</td>\n",
       "      <td>0.910152</td>\n",
       "      <td>0.122956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.399500</td>\n",
       "      <td>0.422062</td>\n",
       "      <td>0.843687</td>\n",
       "      <td>0.843570</td>\n",
       "      <td>0.687731</td>\n",
       "      <td>0.844219</td>\n",
       "      <td>0.843513</td>\n",
       "      <td>0.906958</td>\n",
       "      <td>0.910924</td>\n",
       "      <td>0.121806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.368300</td>\n",
       "      <td>0.424744</td>\n",
       "      <td>0.835671</td>\n",
       "      <td>0.835536</td>\n",
       "      <td>0.673417</td>\n",
       "      <td>0.837434</td>\n",
       "      <td>0.835984</td>\n",
       "      <td>0.903029</td>\n",
       "      <td>0.912238</td>\n",
       "      <td>0.121941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.380300</td>\n",
       "      <td>0.513169</td>\n",
       "      <td>0.827655</td>\n",
       "      <td>0.826484</td>\n",
       "      <td>0.666895</td>\n",
       "      <td>0.838540</td>\n",
       "      <td>0.828431</td>\n",
       "      <td>0.908918</td>\n",
       "      <td>0.916812</td>\n",
       "      <td>0.139513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.362700</td>\n",
       "      <td>0.437731</td>\n",
       "      <td>0.825651</td>\n",
       "      <td>0.823427</td>\n",
       "      <td>0.665120</td>\n",
       "      <td>0.840569</td>\n",
       "      <td>0.824739</td>\n",
       "      <td>0.909074</td>\n",
       "      <td>0.915766</td>\n",
       "      <td>0.131182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.367100</td>\n",
       "      <td>0.511640</td>\n",
       "      <td>0.830995</td>\n",
       "      <td>0.829711</td>\n",
       "      <td>0.674917</td>\n",
       "      <td>0.843201</td>\n",
       "      <td>0.831812</td>\n",
       "      <td>0.910762</td>\n",
       "      <td>0.917272</td>\n",
       "      <td>0.137188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.353100</td>\n",
       "      <td>0.418917</td>\n",
       "      <td>0.849031</td>\n",
       "      <td>0.848929</td>\n",
       "      <td>0.698371</td>\n",
       "      <td>0.849502</td>\n",
       "      <td>0.848869</td>\n",
       "      <td>0.916347</td>\n",
       "      <td>0.919421</td>\n",
       "      <td>0.117690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.327400</td>\n",
       "      <td>0.458490</td>\n",
       "      <td>0.837007</td>\n",
       "      <td>0.836291</td>\n",
       "      <td>0.682073</td>\n",
       "      <td>0.844463</td>\n",
       "      <td>0.837645</td>\n",
       "      <td>0.911419</td>\n",
       "      <td>0.918941</td>\n",
       "      <td>0.129675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.334900</td>\n",
       "      <td>0.404195</td>\n",
       "      <td>0.851035</td>\n",
       "      <td>0.850779</td>\n",
       "      <td>0.703390</td>\n",
       "      <td>0.852652</td>\n",
       "      <td>0.850740</td>\n",
       "      <td>0.920697</td>\n",
       "      <td>0.923223</td>\n",
       "      <td>0.115527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.390700</td>\n",
       "      <td>0.442032</td>\n",
       "      <td>0.837675</td>\n",
       "      <td>0.837675</td>\n",
       "      <td>0.675477</td>\n",
       "      <td>0.837739</td>\n",
       "      <td>0.837739</td>\n",
       "      <td>0.907624</td>\n",
       "      <td>0.913645</td>\n",
       "      <td>0.123763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.400600</td>\n",
       "      <td>0.488858</td>\n",
       "      <td>0.831663</td>\n",
       "      <td>0.830894</td>\n",
       "      <td>0.671561</td>\n",
       "      <td>0.839285</td>\n",
       "      <td>0.832312</td>\n",
       "      <td>0.915788</td>\n",
       "      <td>0.919934</td>\n",
       "      <td>0.133394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.312400</td>\n",
       "      <td>0.545489</td>\n",
       "      <td>0.822311</td>\n",
       "      <td>0.821104</td>\n",
       "      <td>0.656043</td>\n",
       "      <td>0.833031</td>\n",
       "      <td>0.823087</td>\n",
       "      <td>0.909628</td>\n",
       "      <td>0.915508</td>\n",
       "      <td>0.145128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.354200</td>\n",
       "      <td>0.532918</td>\n",
       "      <td>0.820307</td>\n",
       "      <td>0.818178</td>\n",
       "      <td>0.659522</td>\n",
       "      <td>0.838436</td>\n",
       "      <td>0.821309</td>\n",
       "      <td>0.919816</td>\n",
       "      <td>0.922090</td>\n",
       "      <td>0.144113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>0.347400</td>\n",
       "      <td>0.412407</td>\n",
       "      <td>0.860387</td>\n",
       "      <td>0.860132</td>\n",
       "      <td>0.722269</td>\n",
       "      <td>0.862191</td>\n",
       "      <td>0.860081</td>\n",
       "      <td>0.915779</td>\n",
       "      <td>0.922684</td>\n",
       "      <td>0.115350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.350500</td>\n",
       "      <td>0.408195</td>\n",
       "      <td>0.852371</td>\n",
       "      <td>0.852286</td>\n",
       "      <td>0.704975</td>\n",
       "      <td>0.852748</td>\n",
       "      <td>0.852227</td>\n",
       "      <td>0.920705</td>\n",
       "      <td>0.924594</td>\n",
       "      <td>0.115482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>0.305300</td>\n",
       "      <td>0.464343</td>\n",
       "      <td>0.845023</td>\n",
       "      <td>0.844895</td>\n",
       "      <td>0.692162</td>\n",
       "      <td>0.846827</td>\n",
       "      <td>0.845337</td>\n",
       "      <td>0.919765</td>\n",
       "      <td>0.922520</td>\n",
       "      <td>0.123598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.296300</td>\n",
       "      <td>0.414674</td>\n",
       "      <td>0.848363</td>\n",
       "      <td>0.847711</td>\n",
       "      <td>0.700949</td>\n",
       "      <td>0.853109</td>\n",
       "      <td>0.847859</td>\n",
       "      <td>0.927185</td>\n",
       "      <td>0.930284</td>\n",
       "      <td>0.117855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>0.351100</td>\n",
       "      <td>0.392470</td>\n",
       "      <td>0.861723</td>\n",
       "      <td>0.861723</td>\n",
       "      <td>0.723577</td>\n",
       "      <td>0.861788</td>\n",
       "      <td>0.861788</td>\n",
       "      <td>0.929445</td>\n",
       "      <td>0.930758</td>\n",
       "      <td>0.110196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.350400</td>\n",
       "      <td>0.531839</td>\n",
       "      <td>0.826987</td>\n",
       "      <td>0.825590</td>\n",
       "      <td>0.667494</td>\n",
       "      <td>0.839774</td>\n",
       "      <td>0.827827</td>\n",
       "      <td>0.924862</td>\n",
       "      <td>0.924644</td>\n",
       "      <td>0.140718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>0.333700</td>\n",
       "      <td>0.421264</td>\n",
       "      <td>0.850367</td>\n",
       "      <td>0.849860</td>\n",
       "      <td>0.703910</td>\n",
       "      <td>0.853995</td>\n",
       "      <td>0.849927</td>\n",
       "      <td>0.927547</td>\n",
       "      <td>0.929537</td>\n",
       "      <td>0.117243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.305600</td>\n",
       "      <td>0.590067</td>\n",
       "      <td>0.812291</td>\n",
       "      <td>0.809897</td>\n",
       "      <td>0.644387</td>\n",
       "      <td>0.831311</td>\n",
       "      <td>0.813327</td>\n",
       "      <td>0.926309</td>\n",
       "      <td>0.926713</td>\n",
       "      <td>0.149608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18500</td>\n",
       "      <td>0.349200</td>\n",
       "      <td>0.699778</td>\n",
       "      <td>0.777555</td>\n",
       "      <td>0.773176</td>\n",
       "      <td>0.582031</td>\n",
       "      <td>0.803744</td>\n",
       "      <td>0.778820</td>\n",
       "      <td>0.901161</td>\n",
       "      <td>0.906564</td>\n",
       "      <td>0.180131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>0.341400</td>\n",
       "      <td>0.432826</td>\n",
       "      <td>0.854375</td>\n",
       "      <td>0.854357</td>\n",
       "      <td>0.708725</td>\n",
       "      <td>0.854384</td>\n",
       "      <td>0.854341</td>\n",
       "      <td>0.928986</td>\n",
       "      <td>0.929937</td>\n",
       "      <td>0.116348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19500</td>\n",
       "      <td>0.327800</td>\n",
       "      <td>0.584906</td>\n",
       "      <td>0.816967</td>\n",
       "      <td>0.815329</td>\n",
       "      <td>0.648384</td>\n",
       "      <td>0.830664</td>\n",
       "      <td>0.817847</td>\n",
       "      <td>0.914825</td>\n",
       "      <td>0.917467</td>\n",
       "      <td>0.149123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.316900</td>\n",
       "      <td>0.419829</td>\n",
       "      <td>0.853707</td>\n",
       "      <td>0.853592</td>\n",
       "      <td>0.707826</td>\n",
       "      <td>0.854298</td>\n",
       "      <td>0.853528</td>\n",
       "      <td>0.924672</td>\n",
       "      <td>0.928511</td>\n",
       "      <td>0.114927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20500</td>\n",
       "      <td>0.280900</td>\n",
       "      <td>0.438288</td>\n",
       "      <td>0.853039</td>\n",
       "      <td>0.852968</td>\n",
       "      <td>0.707550</td>\n",
       "      <td>0.854255</td>\n",
       "      <td>0.853295</td>\n",
       "      <td>0.925477</td>\n",
       "      <td>0.928466</td>\n",
       "      <td>0.116680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>0.325900</td>\n",
       "      <td>0.441981</td>\n",
       "      <td>0.860387</td>\n",
       "      <td>0.860277</td>\n",
       "      <td>0.722884</td>\n",
       "      <td>0.862189</td>\n",
       "      <td>0.860696</td>\n",
       "      <td>0.930922</td>\n",
       "      <td>0.932370</td>\n",
       "      <td>0.114761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21500</td>\n",
       "      <td>0.279700</td>\n",
       "      <td>0.449463</td>\n",
       "      <td>0.849031</td>\n",
       "      <td>0.848996</td>\n",
       "      <td>0.698974</td>\n",
       "      <td>0.849746</td>\n",
       "      <td>0.849229</td>\n",
       "      <td>0.924654</td>\n",
       "      <td>0.926319</td>\n",
       "      <td>0.116637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.325000</td>\n",
       "      <td>0.476060</td>\n",
       "      <td>0.847027</td>\n",
       "      <td>0.846765</td>\n",
       "      <td>0.697804</td>\n",
       "      <td>0.850359</td>\n",
       "      <td>0.847451</td>\n",
       "      <td>0.922718</td>\n",
       "      <td>0.925201</td>\n",
       "      <td>0.122707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22500</td>\n",
       "      <td>0.285500</td>\n",
       "      <td>0.870074</td>\n",
       "      <td>0.756847</td>\n",
       "      <td>0.747298</td>\n",
       "      <td>0.563595</td>\n",
       "      <td>0.807085</td>\n",
       "      <td>0.758592</td>\n",
       "      <td>0.917087</td>\n",
       "      <td>0.918118</td>\n",
       "      <td>0.204860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>0.348100</td>\n",
       "      <td>0.449410</td>\n",
       "      <td>0.861055</td>\n",
       "      <td>0.860868</td>\n",
       "      <td>0.723095</td>\n",
       "      <td>0.862295</td>\n",
       "      <td>0.860801</td>\n",
       "      <td>0.928369</td>\n",
       "      <td>0.931756</td>\n",
       "      <td>0.114445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23500</td>\n",
       "      <td>0.336100</td>\n",
       "      <td>0.416843</td>\n",
       "      <td>0.862391</td>\n",
       "      <td>0.862353</td>\n",
       "      <td>0.725819</td>\n",
       "      <td>0.863217</td>\n",
       "      <td>0.862602</td>\n",
       "      <td>0.929940</td>\n",
       "      <td>0.933686</td>\n",
       "      <td>0.109527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.319700</td>\n",
       "      <td>0.589937</td>\n",
       "      <td>0.811623</td>\n",
       "      <td>0.808235</td>\n",
       "      <td>0.650657</td>\n",
       "      <td>0.838317</td>\n",
       "      <td>0.812839</td>\n",
       "      <td>0.926786</td>\n",
       "      <td>0.930285</td>\n",
       "      <td>0.151049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24500</td>\n",
       "      <td>0.309000</td>\n",
       "      <td>0.500352</td>\n",
       "      <td>0.846359</td>\n",
       "      <td>0.845712</td>\n",
       "      <td>0.700677</td>\n",
       "      <td>0.853723</td>\n",
       "      <td>0.846986</td>\n",
       "      <td>0.924217</td>\n",
       "      <td>0.928740</td>\n",
       "      <td>0.127514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.274500</td>\n",
       "      <td>0.446852</td>\n",
       "      <td>0.858383</td>\n",
       "      <td>0.858148</td>\n",
       "      <td>0.720513</td>\n",
       "      <td>0.861717</td>\n",
       "      <td>0.858802</td>\n",
       "      <td>0.932831</td>\n",
       "      <td>0.931859</td>\n",
       "      <td>0.115733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25500</td>\n",
       "      <td>0.288500</td>\n",
       "      <td>0.547724</td>\n",
       "      <td>0.840347</td>\n",
       "      <td>0.839449</td>\n",
       "      <td>0.690807</td>\n",
       "      <td>0.849802</td>\n",
       "      <td>0.841060</td>\n",
       "      <td>0.928977</td>\n",
       "      <td>0.929216</td>\n",
       "      <td>0.132467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.341200</td>\n",
       "      <td>0.489769</td>\n",
       "      <td>0.853707</td>\n",
       "      <td>0.853078</td>\n",
       "      <td>0.715663</td>\n",
       "      <td>0.861357</td>\n",
       "      <td>0.854340</td>\n",
       "      <td>0.928980</td>\n",
       "      <td>0.929853</td>\n",
       "      <td>0.122289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26500</td>\n",
       "      <td>0.308300</td>\n",
       "      <td>0.461191</td>\n",
       "      <td>0.857047</td>\n",
       "      <td>0.857047</td>\n",
       "      <td>0.714290</td>\n",
       "      <td>0.857160</td>\n",
       "      <td>0.857129</td>\n",
       "      <td>0.926985</td>\n",
       "      <td>0.929886</td>\n",
       "      <td>0.115305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>0.298000</td>\n",
       "      <td>0.562148</td>\n",
       "      <td>0.831663</td>\n",
       "      <td>0.830404</td>\n",
       "      <td>0.676094</td>\n",
       "      <td>0.843713</td>\n",
       "      <td>0.832474</td>\n",
       "      <td>0.924190</td>\n",
       "      <td>0.924573</td>\n",
       "      <td>0.136853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27500</td>\n",
       "      <td>0.305500</td>\n",
       "      <td>0.510510</td>\n",
       "      <td>0.853707</td>\n",
       "      <td>0.853405</td>\n",
       "      <td>0.711841</td>\n",
       "      <td>0.857683</td>\n",
       "      <td>0.854166</td>\n",
       "      <td>0.925302</td>\n",
       "      <td>0.928294</td>\n",
       "      <td>0.121191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.272100</td>\n",
       "      <td>0.495736</td>\n",
       "      <td>0.853039</td>\n",
       "      <td>0.852470</td>\n",
       "      <td>0.713593</td>\n",
       "      <td>0.859978</td>\n",
       "      <td>0.853643</td>\n",
       "      <td>0.926658</td>\n",
       "      <td>0.929650</td>\n",
       "      <td>0.121627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28500</td>\n",
       "      <td>0.297000</td>\n",
       "      <td>0.448253</td>\n",
       "      <td>0.859719</td>\n",
       "      <td>0.859556</td>\n",
       "      <td>0.720224</td>\n",
       "      <td>0.860736</td>\n",
       "      <td>0.859488</td>\n",
       "      <td>0.929354</td>\n",
       "      <td>0.931997</td>\n",
       "      <td>0.112919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>0.289900</td>\n",
       "      <td>0.458850</td>\n",
       "      <td>0.867735</td>\n",
       "      <td>0.867722</td>\n",
       "      <td>0.735447</td>\n",
       "      <td>0.867734</td>\n",
       "      <td>0.867714</td>\n",
       "      <td>0.927950</td>\n",
       "      <td>0.929044</td>\n",
       "      <td>0.112618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29500</td>\n",
       "      <td>0.287600</td>\n",
       "      <td>0.446038</td>\n",
       "      <td>0.867067</td>\n",
       "      <td>0.866919</td>\n",
       "      <td>0.736921</td>\n",
       "      <td>0.869501</td>\n",
       "      <td>0.867423</td>\n",
       "      <td>0.931722</td>\n",
       "      <td>0.932404</td>\n",
       "      <td>0.112597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.325200</td>\n",
       "      <td>0.500382</td>\n",
       "      <td>0.859719</td>\n",
       "      <td>0.859659</td>\n",
       "      <td>0.720806</td>\n",
       "      <td>0.860843</td>\n",
       "      <td>0.859964</td>\n",
       "      <td>0.929308</td>\n",
       "      <td>0.929845</td>\n",
       "      <td>0.118009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30500</td>\n",
       "      <td>0.266100</td>\n",
       "      <td>0.449735</td>\n",
       "      <td>0.866399</td>\n",
       "      <td>0.866382</td>\n",
       "      <td>0.732777</td>\n",
       "      <td>0.866411</td>\n",
       "      <td>0.866366</td>\n",
       "      <td>0.926303</td>\n",
       "      <td>0.928932</td>\n",
       "      <td>0.110345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31000</td>\n",
       "      <td>0.279800</td>\n",
       "      <td>0.471182</td>\n",
       "      <td>0.861055</td>\n",
       "      <td>0.860894</td>\n",
       "      <td>0.722901</td>\n",
       "      <td>0.862077</td>\n",
       "      <td>0.860825</td>\n",
       "      <td>0.923512</td>\n",
       "      <td>0.927922</td>\n",
       "      <td>0.114744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31500</td>\n",
       "      <td>0.285500</td>\n",
       "      <td>0.478994</td>\n",
       "      <td>0.862391</td>\n",
       "      <td>0.862288</td>\n",
       "      <td>0.726826</td>\n",
       "      <td>0.864133</td>\n",
       "      <td>0.862694</td>\n",
       "      <td>0.930708</td>\n",
       "      <td>0.931815</td>\n",
       "      <td>0.113034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>0.272300</td>\n",
       "      <td>0.507375</td>\n",
       "      <td>0.861055</td>\n",
       "      <td>0.860809</td>\n",
       "      <td>0.726081</td>\n",
       "      <td>0.864602</td>\n",
       "      <td>0.861486</td>\n",
       "      <td>0.928479</td>\n",
       "      <td>0.928906</td>\n",
       "      <td>0.118379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32500</td>\n",
       "      <td>0.297000</td>\n",
       "      <td>0.490620</td>\n",
       "      <td>0.863727</td>\n",
       "      <td>0.863726</td>\n",
       "      <td>0.727501</td>\n",
       "      <td>0.863738</td>\n",
       "      <td>0.863764</td>\n",
       "      <td>0.920668</td>\n",
       "      <td>0.925947</td>\n",
       "      <td>0.114608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33000</td>\n",
       "      <td>0.250800</td>\n",
       "      <td>0.577825</td>\n",
       "      <td>0.840347</td>\n",
       "      <td>0.839280</td>\n",
       "      <td>0.687616</td>\n",
       "      <td>0.847963</td>\n",
       "      <td>0.839703</td>\n",
       "      <td>0.921043</td>\n",
       "      <td>0.924833</td>\n",
       "      <td>0.131036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33500</td>\n",
       "      <td>0.205100</td>\n",
       "      <td>0.468486</td>\n",
       "      <td>0.860387</td>\n",
       "      <td>0.860375</td>\n",
       "      <td>0.720751</td>\n",
       "      <td>0.860380</td>\n",
       "      <td>0.860371</td>\n",
       "      <td>0.917296</td>\n",
       "      <td>0.923306</td>\n",
       "      <td>0.114147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>0.285700</td>\n",
       "      <td>0.502862</td>\n",
       "      <td>0.864395</td>\n",
       "      <td>0.864361</td>\n",
       "      <td>0.729778</td>\n",
       "      <td>0.865179</td>\n",
       "      <td>0.864600</td>\n",
       "      <td>0.926355</td>\n",
       "      <td>0.928486</td>\n",
       "      <td>0.116194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34500</td>\n",
       "      <td>0.269400</td>\n",
       "      <td>0.554616</td>\n",
       "      <td>0.847695</td>\n",
       "      <td>0.847131</td>\n",
       "      <td>0.702534</td>\n",
       "      <td>0.854272</td>\n",
       "      <td>0.848287</td>\n",
       "      <td>0.916356</td>\n",
       "      <td>0.922470</td>\n",
       "      <td>0.129669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>0.289000</td>\n",
       "      <td>0.472138</td>\n",
       "      <td>0.862391</td>\n",
       "      <td>0.862381</td>\n",
       "      <td>0.724762</td>\n",
       "      <td>0.862381</td>\n",
       "      <td>0.862381</td>\n",
       "      <td>0.920026</td>\n",
       "      <td>0.925862</td>\n",
       "      <td>0.114192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35500</td>\n",
       "      <td>0.270900</td>\n",
       "      <td>0.512021</td>\n",
       "      <td>0.854375</td>\n",
       "      <td>0.854219</td>\n",
       "      <td>0.711380</td>\n",
       "      <td>0.856658</td>\n",
       "      <td>0.854724</td>\n",
       "      <td>0.922799</td>\n",
       "      <td>0.922793</td>\n",
       "      <td>0.121427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>0.271000</td>\n",
       "      <td>0.543685</td>\n",
       "      <td>0.849031</td>\n",
       "      <td>0.848691</td>\n",
       "      <td>0.702765</td>\n",
       "      <td>0.853268</td>\n",
       "      <td>0.849507</td>\n",
       "      <td>0.922793</td>\n",
       "      <td>0.923090</td>\n",
       "      <td>0.126520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36500</td>\n",
       "      <td>0.248400</td>\n",
       "      <td>0.709045</td>\n",
       "      <td>0.835671</td>\n",
       "      <td>0.834364</td>\n",
       "      <td>0.684988</td>\n",
       "      <td>0.848589</td>\n",
       "      <td>0.836506</td>\n",
       "      <td>0.923673</td>\n",
       "      <td>0.920340</td>\n",
       "      <td>0.143543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37000</td>\n",
       "      <td>0.242800</td>\n",
       "      <td>0.507138</td>\n",
       "      <td>0.864395</td>\n",
       "      <td>0.864393</td>\n",
       "      <td>0.729065</td>\n",
       "      <td>0.864570</td>\n",
       "      <td>0.864495</td>\n",
       "      <td>0.922413</td>\n",
       "      <td>0.928882</td>\n",
       "      <td>0.114268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37500</td>\n",
       "      <td>0.266400</td>\n",
       "      <td>0.636032</td>\n",
       "      <td>0.841015</td>\n",
       "      <td>0.840072</td>\n",
       "      <td>0.692653</td>\n",
       "      <td>0.850968</td>\n",
       "      <td>0.841746</td>\n",
       "      <td>0.926668</td>\n",
       "      <td>0.926690</td>\n",
       "      <td>0.135256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38000</td>\n",
       "      <td>0.229900</td>\n",
       "      <td>0.533009</td>\n",
       "      <td>0.860387</td>\n",
       "      <td>0.860232</td>\n",
       "      <td>0.723522</td>\n",
       "      <td>0.862782</td>\n",
       "      <td>0.860742</td>\n",
       "      <td>0.922393</td>\n",
       "      <td>0.925762</td>\n",
       "      <td>0.116927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38500</td>\n",
       "      <td>0.241100</td>\n",
       "      <td>0.537198</td>\n",
       "      <td>0.853039</td>\n",
       "      <td>0.852689</td>\n",
       "      <td>0.711050</td>\n",
       "      <td>0.857534</td>\n",
       "      <td>0.853527</td>\n",
       "      <td>0.917421</td>\n",
       "      <td>0.918939</td>\n",
       "      <td>0.124790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39000</td>\n",
       "      <td>0.236300</td>\n",
       "      <td>0.491990</td>\n",
       "      <td>0.868403</td>\n",
       "      <td>0.868375</td>\n",
       "      <td>0.737700</td>\n",
       "      <td>0.869104</td>\n",
       "      <td>0.868597</td>\n",
       "      <td>0.923824</td>\n",
       "      <td>0.926995</td>\n",
       "      <td>0.112976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39500</td>\n",
       "      <td>0.225000</td>\n",
       "      <td>0.915203</td>\n",
       "      <td>0.798263</td>\n",
       "      <td>0.793601</td>\n",
       "      <td>0.630131</td>\n",
       "      <td>0.831297</td>\n",
       "      <td>0.799629</td>\n",
       "      <td>0.915852</td>\n",
       "      <td>0.911158</td>\n",
       "      <td>0.179151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>0.218300</td>\n",
       "      <td>0.586453</td>\n",
       "      <td>0.861055</td>\n",
       "      <td>0.860961</td>\n",
       "      <td>0.724005</td>\n",
       "      <td>0.862659</td>\n",
       "      <td>0.861347</td>\n",
       "      <td>0.921232</td>\n",
       "      <td>0.921811</td>\n",
       "      <td>0.122280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40500</td>\n",
       "      <td>0.285600</td>\n",
       "      <td>0.620954</td>\n",
       "      <td>0.851035</td>\n",
       "      <td>0.850368</td>\n",
       "      <td>0.710562</td>\n",
       "      <td>0.858919</td>\n",
       "      <td>0.851680</td>\n",
       "      <td>0.924953</td>\n",
       "      <td>0.920723</td>\n",
       "      <td>0.130775</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[13], line 99\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     97\u001b[0m callback \u001b[38;5;241m=\u001b[39m EvalAndSaveCallback(trainer)\n\u001b[1;32m     98\u001b[0m trainer\u001b[38;5;241m.\u001b[39madd_callback(callback)\n\u001b[0;32m---> 99\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training_args\u001b[38;5;241m.\u001b[39msave_model:\n\u001b[1;32m    102\u001b[0m     trainer\u001b[38;5;241m.\u001b[39msave_state()\n",
      "File \u001b[0;32m/common/zhanh/anaconda3/envs/dna_lora/lib/python3.8/site-packages/transformers/trainer.py:1662\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1657\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[1;32m   1659\u001b[0m inner_training_loop \u001b[38;5;241m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1660\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_training_loop, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size, args\u001b[38;5;241m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1661\u001b[0m )\n\u001b[0;32m-> 1662\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1663\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1664\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1665\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1666\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1667\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/common/zhanh/anaconda3/envs/dna_lora/lib/python3.8/site-packages/transformers/trainer.py:1931\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1928\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1929\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[0;32m-> 1931\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1932\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1933\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1934\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1935\u001b[0m ):\n\u001b[1;32m   1936\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1937\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[1;32m   1938\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2b9170",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Create a pattern\n",
    "pattern = r\"bert\\.encoder\\.layer\\.\\d+\\.mlp\\.wo\"\n",
    "\n",
    "# Test string\n",
    "test_string = \"bert.encoder.layer.11.mlp.wo\"\n",
    "\n",
    "# Use re.search() to find the pattern in the test string\n",
    "match = re.search(pattern, test_string)\n",
    "\n",
    "if match:\n",
    "  print(\"Match found!\")\n",
    "else:\n",
    "  print(\"Match not found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61853275",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "# Set the font style and size\n",
    "mpl.rcParams['font.family'] = 'Times New Roman'\n",
    "mpl.rcParams['font.size'] = 16\n",
    "\n",
    "# Define the data\n",
    "categories = ['Layers 0-3', 'Layers 4-7', 'Layers 8-11']\n",
    "values = [0.676, 0.689, 0.791]\n",
    "\n",
    "\n",
    "# Create the bar plot\n",
    "bars = plt.bar(categories, values, color='skyblue', width=0.5)\n",
    "\n",
    "for bar in bars:\n",
    "    yval = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2.0, yval, yval, va='bottom',ha='center')  # va='bottom' to make the text appear above the bar\n",
    "\n",
    "\n",
    "\n",
    "# Provide labels and title\n",
    "plt.xlabel('Selected layers')\n",
    "plt.ylabel('F-1')\n",
    "#plt.title('Bar Plot Example')\n",
    "# Set y scale\n",
    "plt.ylim(0, 1)  # set y scale from 0 to 20\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94662245",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "# Set the font style and size\n",
    "mpl.rcParams['font.family'] = 'Times New Roman'\n",
    "mpl.rcParams['font.size'] = 16\n",
    "\n",
    "# Define the data\n",
    "categories = [r'$W_q$, $W_k$, $W_v$', 'mlp', 'dense']\n",
    "values = [0.666, 0.617, 0.653]\n",
    "\n",
    "\n",
    "# Create the bar plot\n",
    "bars = plt.bar(categories, values, color='skyblue', width=0.5)\n",
    "\n",
    "for bar in bars:\n",
    "    yval = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2.0, yval, yval, va='bottom',ha='center')  # va='bottom' to make the text appear above the bar\n",
    "\n",
    "\n",
    "\n",
    "# Provide labels and title\n",
    "plt.xlabel('Selected matrices')\n",
    "plt.ylabel('F-1')\n",
    "#plt.title('Bar Plot Example')\n",
    "# Set y scale\n",
    "plt.ylim(0, 1)  # set y scale from 0 to 20\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279a0e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize a sentence\n",
    "sentence = \"This is a sample sentence.\"\n",
    "inputs = tokenizer(sentence, return_tensors='pt')\n",
    "print(inputs)\n",
    "# Get the embeddings for the input IDs\n",
    "embeddings = model.embeddings.word_embeddings(inputs['input_ids'])\n",
    "\n",
    "print(embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be16700",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718f3f78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DNA LORA",
   "language": "python",
   "name": "dna_lora"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
